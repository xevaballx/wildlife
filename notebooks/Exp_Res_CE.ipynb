{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6y23yCuMBwj",
    "tags": []
   },
   "source": [
    "## Resnet50 Cross Entropy Loss Experiment (no reweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKiaXIRuMBwn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install wandb -qU\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the current working directory\n",
    "notebook_dir = notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))  \n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..')) \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from src import (get_transforms, load_data, split_data, set_seeds, \n",
    "                 verify_splits, verify_data, plot_species_grid,\n",
    "                 verify_loader_transforms)\n",
    "from src.data_utils import ImagesDataset\n",
    "from src.models import build_resnet50_basic\n",
    "from src.train import setup_training, evaluate_low_log, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your directory is set up properly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set up your experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this notebook. Rename it, but keep it in `notebooks/`. To update any settings, params, and/or hyperparams make a copy of `configs/default.yaml`, rename it and call your new `.yaml` below. Be sure to keep it in `configs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the YAML file relative to the notebook's location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# You need to update this path to your new .yaml file\n",
    "config_path = os.path.join(notebook_dir, \"../configs/res_CE.yaml\")\n",
    "\n",
    "# Load the YAML file\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "device = config[\"device\"]\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Note: your data file should be hidden in the repo (.gitignore) but make sure to set it up locally like:\n",
    "\n",
    "`wildlife/data/givens/test_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features.csv`\n",
    "\n",
    "`wildlife/data/givens/test_features.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features, test_features, train_labels, species_labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transforms, val_transforms = get_transforms(config)\n",
    "# print(train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNzMhcTGMBwp"
   },
   "source": [
    "#### Split into train and evaluation sets\n",
    "\n",
    "We need to ensure that sites are mutually exclusive between the training and validation sets, meaning no site should appear in both sets. This ensures a proper stratification based on site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "X_train, X_val, y_train, y_val = split_data(\n",
    "    train_features, train_labels, type='sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (Optional)\n",
    "# verify_splits(X_train, y_train, X_val,  y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImagesDataset(\n",
    "    features=X_train, \n",
    "    labels=y_train, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset = ImagesDataset(\n",
    "    features=X_val, \n",
    "    labels=y_val, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "if device==\"cuda\":\n",
    "    pin=False\n",
    "else:\n",
    "    pin=True\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=pin)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify transformations in dataloaders (Optional)\n",
    "# verify_loader_transforms(train_loader, title_type='train')\n",
    "# verify_loader_transforms(val_loader, title_type='validate')\n",
    "\n",
    "# set_seeds(config[\"experiment\"]['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes for verification (Optional)\n",
    "# print(f\"Training set: {len(train_dataset)} samples\")\n",
    "# print(f\"Validation set: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPwe5YFjMBwv",
    "tags": []
   },
   "source": [
    "### **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "Note: If you build a new model, add it to `models.py` and update the block below. And update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "model =  build_resnet50_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"],\n",
    "    freeze_backbone = config[\"model\"][\"freeze_backbone\"]\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your criterion and optimizer\n",
    "Note: If needed up date these in `train.py` and update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yf3bCjmwMBwv",
    "outputId": "e0b47689-3576-4c08-c1f8-fb4515032200"
   },
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "class_counts = y_train.sum(axis=0).values\n",
    "# print(class_counts)\n",
    "\n",
    "criterion, optimizer = setup_training(\n",
    "        model, \n",
    "        criterion=config[\"train\"][\"criterion\"],\n",
    "        optimizer=config[\"train\"][\"optimizer\"], \n",
    "        lr=config[\"train\"][\"lr\"], \n",
    "        momentum=config[\"train\"][\"momentum\"],\n",
    "        gamma=config[\"train\"][\"gamma\"],\n",
    "        alpha=config[\"train\"][\"alpha\"],\n",
    "        device=device,\n",
    "        cls_num_list=class_counts)\n",
    "print(config[\"train\"][\"alpha\"])\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"wildlife\")\n",
    "cfg = wandb.config\n",
    "cfg.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the train / eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []  # To store average training loss per epoch\n",
    "val_losses = []    # To store validation loss per epoch\n",
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    # Training step\n",
    "    avg_train_loss = train(model, \n",
    "                                     train_loader, \n",
    "                                     criterion, \n",
    "                                     optimizer, \n",
    "                                     epoch, config, device=device)\n",
    "    train_losses.append(avg_train_loss)  # Store avg training loss\n",
    "    print(f\"Epoch {epoch+1}/{config[\"train\"][\"epochs\"]} - Avg Train Loss: {\n",
    "        avg_train_loss:.12f}\")\n",
    "    \n",
    "    # Evaluation step\n",
    "    eval_metrics = evaluate_low_log(model, val_loader, criterion, config, epoch+1, device=device)\n",
    "    val_losses.append(eval_metrics[\"loss\"])  # Store validation loss\n",
    "    print(f\"Epoch {epoch+1}/{config[\"train\"][\"epochs\"]} - Eval Loss: {\n",
    "        eval_metrics['loss']:.12f}, Eval Acc: {eval_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "wandb.log({\"duration\": duration})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done logging or you want to run the experiment again, finish with the block below. But if you think you might want to submit this run to the competition, don't finish logging until the end once you've added the competition score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore Experiment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to explore and potentially submit your results \n",
    "explore = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Labels from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    print(\"True labels (training):\")\n",
    "    print(y_train.idxmax(axis=1).value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True and Predicated Labels from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # Extract predictions and true labels from eval_metrics\n",
    "    all_preds = eval_metrics[\"all_preds\"]\n",
    "    all_labels = eval_metrics[\"all_labels\"]\n",
    "\n",
    "    # Convert all_preds to DataFrame and map to class names\n",
    "    preds_df = pd.DataFrame(all_preds, columns=[\"predicted_class\"])\n",
    "    preds_df[\"predicted_label\"] = preds_df[\"predicted_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Convert all_labels to DataFrame and map to class names\n",
    "    labels_df = pd.DataFrame(all_labels, columns=[\"true_class\"])\n",
    "    labels_df[\"true_label\"] = labels_df[\"true_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Combine predictions and true labels for analysis\n",
    "    results_df = pd.concat([preds_df, labels_df], axis=1)\n",
    "\n",
    "    # Display value counts for predicted and true labels\n",
    "    print(\"Predicted labels (eval):\")\n",
    "    print(results_df[\"predicted_label\"].value_counts())\n",
    "\n",
    "    print(\"\\nTrue labels (eval):\")\n",
    "    print(results_df[\"true_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:    \n",
    "    per_class_accuracy = results_df.groupby(\"true_label\").apply(\n",
    "        lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).mean(), \n",
    "    )\n",
    "    print(\"Per-Class Accuracy:\")\n",
    "    print(per_class_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "    eval_true = pd.Series(all_labels).apply(lambda x: species_labels[x])\n",
    "    eval_predictions = pd.Series(all_preds).apply(lambda x: species_labels[x])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(\n",
    "        eval_true,\n",
    "        eval_predictions,\n",
    "        ax=ax,\n",
    "        xticks_rotation=90,\n",
    "        colorbar=True,\n",
    "        normalize='true'\n",
    "    )\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
