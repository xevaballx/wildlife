{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6y23yCuMBwj",
    "tags": []
   },
   "source": [
    "## Experiment_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKiaXIRuMBwn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install wandb -qU\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the current working directory\n",
    "notebook_dir = notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))  \n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..')) \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from ensemble_classifier import (get_transforms, load_data, split_data, set_seeds, \n",
    "                 verify_splits, verify_data, plot_species_grid,\n",
    "                 verify_loader_transforms)\n",
    "from ensemble_classifier.data_utils import ImagesDataset\n",
    "from ensemble_classifier.models import build_resnet50_basic, build_efficientnet_v2_basic\n",
    "from ensemble_classifier.train import setup_training, evaluate, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your directory is set up properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree ../ -L 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree ../data/ -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set up your experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this notebook. Rename it, but keep it in `notebooks/`. To update any settings, params, and/or hyperparams make a copy of `configs/default.yaml`, rename it and call your new `.yaml` below. Be sure to keep it in `configs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the YAML file relative to the notebook's location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# You need to update this path to your new .yaml file\n",
    "config_path = os.path.join(notebook_dir, \"../configs/default_cuda.yaml\")\n",
    "\n",
    "# Load the YAML file\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "device = config[\"device\"]\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Note: your data file should be hidden in the repo (.gitignore) but make sure to set it up locally like:\n",
    "\n",
    "`wildlife/data/givens/test_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features.csv`\n",
    "\n",
    "`wildlife/data/givens/test_features.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features, test_features, train_labels, species_labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transforms, val_transforms = get_transforms(config)\n",
    "# print(train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNzMhcTGMBwp"
   },
   "source": [
    "#### Split into train and evaluation sets\n",
    "\n",
    "We need to ensure that sites are mutually exclusive between the training and validation sets, meaning no site should appear in both sets. This ensures a proper stratification based on site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "X_train, X_val, y_train, y_val = split_data(\n",
    "    train_features, train_labels, type='sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (Optional)\n",
    "# verify_splits(X_train, y_train, X_val,  y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImagesDataset(\n",
    "    features=X_train, \n",
    "    labels=y_train, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "\n",
    "val_dataset = ImagesDataset(\n",
    "    features=X_val, \n",
    "    labels=y_val, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "if device==\"cuda\":\n",
    "    pin=False\n",
    "else:\n",
    "    pin=True\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=pin)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify transformations in dataloaders (Optional)\n",
    "# verify_loader_transforms(train_loader, title_type='train')\n",
    "# verify_loader_transforms(val_loader, title_type='validate')\n",
    "\n",
    "# set_seeds(config[\"experiment\"]['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes for verification (Optional)\n",
    "# print(f\"Training set: {len(train_dataset)} samples\")\n",
    "# print(f\"Validation set: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPwe5YFjMBwv",
    "tags": []
   },
   "source": [
    "### **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "Note: If you build a new model, add it to `models.py` and update the block below. And update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_antalope_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_antelope_duiker_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_bird_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_bird_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_blank_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_blank_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_civet_genet_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_civet_genet_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_hog_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_hog_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_leopard_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_leopard_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_monkey_prosimian_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_monkey_prosimian_model.pth\"))\n",
      "C:\\Users\\kseeg\\AppData\\Local\\Temp\\ipykernel_28936\\2479173825.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_rodent_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_rodent_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "# model = build_efficientnet_v2_basic(\n",
    "#     num_classes = config[\"model\"][\"num_classes\"],\n",
    "#     hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "#     dropout = config[\"model\"][\"dropout\"] \n",
    "# )\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "pretrained_antalope_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_bird_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_blank_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_civet_genet_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_hog_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_leopard_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_monkey_prosimian_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_rodent_model = build_resnet50_basic(\n",
    "    # num_classes = config[\"model\"][\"num_classes\"],\n",
    "    num_classes = 1,\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "\n",
    "pretrained_antalope_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_antelope_duiker_model.pth\"))\n",
    "pretrained_bird_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_bird_model.pth\"))\n",
    "pretrained_blank_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_blank_model.pth\"))\n",
    "pretrained_civet_genet_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_civet_genet_model.pth\"))\n",
    "pretrained_hog_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_hog_model.pth\"))\n",
    "pretrained_leopard_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_leopard_model.pth\"))\n",
    "pretrained_monkey_prosimian_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_monkey_prosimian_model.pth\"))\n",
    "pretrained_rodent_model.load_state_dict(torch.load(\"../pretrained_models/pretrained_rodent_model.pth\"))\n",
    "\n",
    "class EnsembleClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_antalope, model_bird, model_blank, model_civet_genet, model_hog, model_leopard, model_monkey_prosimian, model_rodent, output_class_count = 8):\n",
    "            super(EnsembleClassifier, self).__init__()\n",
    "\n",
    "            self.model_antalope = model_antalope\n",
    "            self.model_bird = model_bird\n",
    "            self.model_blank = model_blank\n",
    "            self.model_civet_genet = model_civet_genet\n",
    "            self.model_hog = model_hog\n",
    "            self.model_leopard = model_leopard\n",
    "            self.model_monkey_prosimian = model_monkey_prosimian\n",
    "            self.model_rodent = model_rodent\n",
    "            \n",
    "            self.lin1 = torch.nn.Linear(8,16)\n",
    "            self.relu = torch.nn.ReLU(inplace=True)\n",
    "            self.lin2 = torch.nn.Linear(16,64)\n",
    "            self.lin3 = torch.nn.Linear(64,16)\n",
    "            self.lin4 = torch.nn.Linear(16,8)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # out_antelope = F.softmax(self.model_antalope(X), dim=1)\n",
    "            # out_bird = F.softmax(self.model_bird(X), dim=1)\n",
    "            # out_blank = F.softmax(self.model_blank(X), dim=1)\n",
    "            # out_civet_genet = F.softmax(self.model_civet_genet(X), dim=1)\n",
    "            # out_hog = F.softmax(self.model_hog(X), dim=1)\n",
    "            # out_leopard = F.softmax(self.model_leopard(X), dim=1)\n",
    "            # out_monkey_prosimian = F.softmax(self.model_monkey_prosimian(X), dim=1)\n",
    "            # out_rodent = F.softmax(self.model_rodent(X), dim=1)\n",
    "\n",
    "            out_antelope = (self.model_antalope(X))\n",
    "            out_bird = (self.model_bird(X))\n",
    "            out_blank = (self.model_blank(X))\n",
    "            out_civet_genet = (self.model_civet_genet(X))\n",
    "            out_hog = (self.model_hog(X))\n",
    "            out_leopard = (self.model_leopard(X))\n",
    "            out_monkey_prosimian = (self.model_monkey_prosimian(X))\n",
    "            out_rodent = (self.model_rodent(X))\n",
    "\n",
    "    \n",
    "        # print(f\"out Antelope is: {out_antelope}\")\n",
    "        # print(f\"out bird is: {out_bird}\")\n",
    "        # print(f\"Out blank is: {out_blank}\")\n",
    "        # print(f\"Out_civet genet: {out_civet_genet}\")\n",
    "        # print(f\"outpu_hog: {out_hog}\")\n",
    "        # print(f\"out leopard: {out_leopard}\")\n",
    "        # print(f\"Out monkey: {out_monkey_prosimian}\")\n",
    "        # print(f\"out rodent: {out_rodent}\")\n",
    "        \n",
    "        #mega_X = torch.stack((out_antelope[:,1], out_bird[:,1], out_blank[:,1], out_civet_genet[:,1], out_hog[:,1], out_leopard[:,1], out_monkey_prosimian[:,1], out_rodent[:,1]), dim = 0).T\n",
    "        \n",
    "        mega_X = torch.stack((out_antelope, out_bird, out_blank, out_civet_genet, out_hog, out_leopard, out_monkey_prosimian, out_rodent), dim = 0).T.squeeze()\n",
    "        \n",
    "\n",
    "        out = self.lin1(mega_X)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin4(out)\n",
    "\n",
    "        #print(\"output at the model output:\")\n",
    "\n",
    "        #print(out[0])        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = EnsembleClassifier(pretrained_antalope_model, pretrained_bird_model, pretrained_blank_model, pretrained_civet_genet_model, pretrained_hog_model, pretrained_leopard_model, pretrained_monkey_prosimian_model, pretrained_rodent_model, output_class_count=8)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your criterion and optimizer\n",
    "Note: If needed up date these in `train.py` and update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yf3bCjmwMBwv",
    "outputId": "e0b47689-3576-4c08-c1f8-fb4515032200"
   },
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "criterion, optimizer = setup_training(\n",
    "        model, \n",
    "        criterion=config[\"train\"][\"criterion\"],\n",
    "        optimizer=config[\"train\"][\"optimizer\"], \n",
    "        lr=config[\"train\"][\"lr\"], \n",
    "        momentum=config[\"train\"][\"momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ¨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"wildlife\")\n",
    "cfg = wandb.config\n",
    "cfg.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the train / eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for epoch 1\n",
      "out Antelope is: tensor([[-0.2828],\n",
      "        [-0.3098],\n",
      "        [-0.3793],\n",
      "        [-0.3468],\n",
      "        [-0.2925],\n",
      "        [-0.7101],\n",
      "        [-0.3315],\n",
      "        [-0.5495],\n",
      "        [-1.3941],\n",
      "        [-0.2649],\n",
      "        [-0.8072],\n",
      "        [-2.5485],\n",
      "        [-0.8055],\n",
      "        [-2.4026],\n",
      "        [-0.8432],\n",
      "        [-1.0788]], device='cuda:0')\n",
      "out bird is: tensor([[ 0.8407],\n",
      "        [ 2.8879],\n",
      "        [ 0.5900],\n",
      "        [ 0.4177],\n",
      "        [ 0.7399],\n",
      "        [ 2.9019],\n",
      "        [-2.7489],\n",
      "        [-0.4608],\n",
      "        [-9.9987],\n",
      "        [ 1.1584],\n",
      "        [-1.2694],\n",
      "        [-5.0648],\n",
      "        [-0.0585],\n",
      "        [ 0.4437],\n",
      "        [-6.3306],\n",
      "        [-0.6663]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.0677],\n",
      "        [-0.2248],\n",
      "        [ 1.3273],\n",
      "        [ 1.0728],\n",
      "        [-0.1083],\n",
      "        [ 0.0671],\n",
      "        [-0.6014],\n",
      "        [ 0.6202],\n",
      "        [ 0.4405],\n",
      "        [-0.1659],\n",
      "        [-1.0342],\n",
      "        [-0.0178],\n",
      "        [ 0.5781],\n",
      "        [ 1.2636],\n",
      "        [ 0.1454],\n",
      "        [-0.2665]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -3.5558],\n",
      "        [ -7.6977],\n",
      "        [ -3.1390],\n",
      "        [-12.3793],\n",
      "        [  2.6195],\n",
      "        [ -2.0020],\n",
      "        [  3.4964],\n",
      "        [-11.3340],\n",
      "        [  5.6537],\n",
      "        [ -5.9636],\n",
      "        [  1.5918],\n",
      "        [ -9.1105],\n",
      "        [ -3.7018],\n",
      "        [ -8.8881],\n",
      "        [  8.1009],\n",
      "        [ -8.5505]], device='cuda:0')\n",
      "outpu_hog: tensor([[-0.5177],\n",
      "        [-3.0226],\n",
      "        [-5.3064],\n",
      "        [-5.6546],\n",
      "        [ 0.0252],\n",
      "        [ 0.3583],\n",
      "        [ 0.4852],\n",
      "        [-0.2350],\n",
      "        [-0.8123],\n",
      "        [-1.6839],\n",
      "        [19.8395],\n",
      "        [ 1.0594],\n",
      "        [-3.4668],\n",
      "        [ 3.2387],\n",
      "        [-6.4392],\n",
      "        [-1.0659]], device='cuda:0')\n",
      "out leopard: tensor([[-1.8055],\n",
      "        [-2.1307],\n",
      "        [-2.0513],\n",
      "        [-1.8351],\n",
      "        [-2.0644],\n",
      "        [-1.9586],\n",
      "        [-1.9518],\n",
      "        [-2.1529],\n",
      "        [-1.8214],\n",
      "        [-1.9959],\n",
      "        [-1.7624],\n",
      "        [-2.1218],\n",
      "        [-1.5750],\n",
      "        [-1.8085],\n",
      "        [-1.8666],\n",
      "        [-2.4256]], device='cuda:0')\n",
      "Out monkey: tensor([[  0.3472],\n",
      "        [  1.4457],\n",
      "        [  0.6600],\n",
      "        [  0.2984],\n",
      "        [ -0.0351],\n",
      "        [  1.0165],\n",
      "        [ -4.6102],\n",
      "        [ -0.2399],\n",
      "        [-13.9938],\n",
      "        [  0.5166],\n",
      "        [ -2.2666],\n",
      "        [-14.1053],\n",
      "        [  2.0745],\n",
      "        [  2.0837],\n",
      "        [ -3.5193],\n",
      "        [  0.5950]], device='cuda:0')\n",
      "out rodent: tensor([[ 2.1116],\n",
      "        [ 0.2683],\n",
      "        [-2.0404],\n",
      "        [-2.7398],\n",
      "        [ 6.3579],\n",
      "        [ 1.2052],\n",
      "        [ 0.7630],\n",
      "        [-5.4994],\n",
      "        [ 3.9484],\n",
      "        [-0.8794],\n",
      "        [-1.5308],\n",
      "        [-5.8044],\n",
      "        [-4.5131],\n",
      "        [-5.0607],\n",
      "        [-0.2952],\n",
      "        [ 0.7217]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.2828,  0.8407, -0.0677, -3.5558, -0.5177, -1.8055,  0.3472,  2.1116],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1222,  0.0176, -0.1404,  0.1662,  0.1972, -0.1348, -0.1044,  0.1136],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.2465],\n",
      "        [-0.7422],\n",
      "        [-1.8480],\n",
      "        [-0.2707],\n",
      "        [-1.3739],\n",
      "        [-0.2750],\n",
      "        [-0.6145],\n",
      "        [-0.2590],\n",
      "        [-0.3636],\n",
      "        [-1.0058],\n",
      "        [-0.5477],\n",
      "        [-3.1948],\n",
      "        [-1.5954],\n",
      "        [-0.9882],\n",
      "        [-0.9865],\n",
      "        [-0.2491]], device='cuda:0')\n",
      "out bird is: tensor([[ 0.3769],\n",
      "        [-2.4551],\n",
      "        [-2.3905],\n",
      "        [-0.0171],\n",
      "        [ 0.1899],\n",
      "        [ 0.0521],\n",
      "        [ 2.5245],\n",
      "        [ 1.0299],\n",
      "        [-2.8660],\n",
      "        [-1.5854],\n",
      "        [-1.2285],\n",
      "        [-2.3307],\n",
      "        [-2.0832],\n",
      "        [-0.1913],\n",
      "        [ 0.1389],\n",
      "        [ 2.2404]], device='cuda:0')\n",
      "Out blank is: tensor([[ 0.7114],\n",
      "        [-0.0969],\n",
      "        [-1.5367],\n",
      "        [ 0.3166],\n",
      "        [-0.0729],\n",
      "        [ 0.1672],\n",
      "        [ 0.0952],\n",
      "        [ 0.7062],\n",
      "        [-0.0518],\n",
      "        [-0.4225],\n",
      "        [-0.2751],\n",
      "        [ 0.0575],\n",
      "        [ 0.9724],\n",
      "        [ 0.2851],\n",
      "        [ 2.3396],\n",
      "        [-0.1419]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -8.1571],\n",
      "        [  2.9287],\n",
      "        [  0.6059],\n",
      "        [ -4.7032],\n",
      "        [ -5.9734],\n",
      "        [-13.9080],\n",
      "        [ -4.2561],\n",
      "        [ -1.1413],\n",
      "        [  6.9076],\n",
      "        [  0.8495],\n",
      "        [ -4.8963],\n",
      "        [ -6.4403],\n",
      "        [-19.5015],\n",
      "        [  3.9350],\n",
      "        [ -0.3937],\n",
      "        [ -7.9652]], device='cuda:0')\n",
      "outpu_hog: tensor([[ -1.4754],\n",
      "        [ 12.4779],\n",
      "        [  3.7163],\n",
      "        [  0.1451],\n",
      "        [ -1.6482],\n",
      "        [ -2.9088],\n",
      "        [ -0.9719],\n",
      "        [ -3.0067],\n",
      "        [-17.5652],\n",
      "        [  1.4749],\n",
      "        [  3.8608],\n",
      "        [ -4.4780],\n",
      "        [  0.1805],\n",
      "        [  9.5128],\n",
      "        [  4.6419],\n",
      "        [ -0.0462]], device='cuda:0')\n",
      "out leopard: tensor([[-2.0887],\n",
      "        [-2.1351],\n",
      "        [-1.8861],\n",
      "        [-1.9316],\n",
      "        [-2.0363],\n",
      "        [-2.3487],\n",
      "        [-1.8512],\n",
      "        [-1.6005],\n",
      "        [-1.8389],\n",
      "        [-1.7655],\n",
      "        [-1.5773],\n",
      "        [-1.8475],\n",
      "        [-2.0459],\n",
      "        [-1.5331],\n",
      "        [-1.5704],\n",
      "        [-1.8524]], device='cuda:0')\n",
      "Out monkey: tensor([[  1.5092],\n",
      "        [ -5.6836],\n",
      "        [ -1.9141],\n",
      "        [  1.7756],\n",
      "        [  0.4459],\n",
      "        [  1.6763],\n",
      "        [  0.8204],\n",
      "        [  0.6472],\n",
      "        [-18.7279],\n",
      "        [ -1.5047],\n",
      "        [  1.6089],\n",
      "        [ -9.0066],\n",
      "        [ -0.6003],\n",
      "        [ -3.0835],\n",
      "        [ -0.8419],\n",
      "        [ -0.2154]], device='cuda:0')\n",
      "out rodent: tensor([[-0.6168],\n",
      "        [-3.0816],\n",
      "        [-0.5981],\n",
      "        [-1.9652],\n",
      "        [-0.8361],\n",
      "        [-8.2066],\n",
      "        [ 2.6766],\n",
      "        [-0.2051],\n",
      "        [ 5.7507],\n",
      "        [ 5.2149],\n",
      "        [-2.1554],\n",
      "        [-6.3689],\n",
      "        [-2.7713],\n",
      "        [-3.3800],\n",
      "        [ 1.2020],\n",
      "        [ 1.6000]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.2465,  0.3769,  0.7114, -8.1571, -1.4754, -2.0887,  1.5092, -0.6168],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.2006, -0.0372, -0.0745,  0.1870,  0.1837, -0.1907, -0.1553,  0.1080],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.4140],\n",
      "        [-2.4003],\n",
      "        [-0.1288],\n",
      "        [-0.3014],\n",
      "        [-0.5724],\n",
      "        [-2.7970],\n",
      "        [-0.5642],\n",
      "        [-0.2931],\n",
      "        [-1.2119],\n",
      "        [-1.1709],\n",
      "        [-0.2622],\n",
      "        [-1.2337],\n",
      "        [-0.2762],\n",
      "        [-0.9249],\n",
      "        [-0.9545],\n",
      "        [-0.9976]], device='cuda:0')\n",
      "out bird is: tensor([[ 1.5335e+00],\n",
      "        [ 1.9421e-01],\n",
      "        [-1.0917e+01],\n",
      "        [ 2.5276e-01],\n",
      "        [-2.2698e+00],\n",
      "        [-2.6287e+00],\n",
      "        [ 5.0101e-01],\n",
      "        [-6.3560e-01],\n",
      "        [ 1.4938e+00],\n",
      "        [-4.2604e-01],\n",
      "        [ 1.3507e+00],\n",
      "        [-1.8657e-01],\n",
      "        [-6.6000e-03],\n",
      "        [ 1.4407e+00],\n",
      "        [ 1.2196e+00],\n",
      "        [-2.1359e+00]], device='cuda:0')\n",
      "Out blank is: tensor([[ 0.4183],\n",
      "        [-0.0820],\n",
      "        [-1.6773],\n",
      "        [ 0.3017],\n",
      "        [-0.1763],\n",
      "        [-0.0388],\n",
      "        [ 1.0042],\n",
      "        [ 0.2712],\n",
      "        [ 0.0505],\n",
      "        [-0.4549],\n",
      "        [-0.1422],\n",
      "        [ 0.7537],\n",
      "        [ 0.6953],\n",
      "        [-0.0289],\n",
      "        [ 0.5895],\n",
      "        [-0.4656]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -6.4058],\n",
      "        [ -6.7254],\n",
      "        [  7.4651],\n",
      "        [ -4.4701],\n",
      "        [  0.8883],\n",
      "        [  5.6537],\n",
      "        [-22.0392],\n",
      "        [ -4.4336],\n",
      "        [  3.2677],\n",
      "        [  3.1997],\n",
      "        [ -7.9777],\n",
      "        [-12.1037],\n",
      "        [ -7.7876],\n",
      "        [ -4.7852],\n",
      "        [ -5.3136],\n",
      "        [ -1.5044]], device='cuda:0')\n",
      "outpu_hog: tensor([[ -3.6411],\n",
      "        [  1.4528],\n",
      "        [ -8.8316],\n",
      "        [  1.3119],\n",
      "        [  0.6189],\n",
      "        [  6.0495],\n",
      "        [-11.9263],\n",
      "        [  2.2992],\n",
      "        [  3.4803],\n",
      "        [ 11.2950],\n",
      "        [ -2.0326],\n",
      "        [ -0.4373],\n",
      "        [ -1.6360],\n",
      "        [ -1.9004],\n",
      "        [  0.3642],\n",
      "        [ -3.8482]], device='cuda:0')\n",
      "out leopard: tensor([[-2.0769],\n",
      "        [-1.9509],\n",
      "        [-2.0457],\n",
      "        [-2.1442],\n",
      "        [-1.8841],\n",
      "        [-1.6862],\n",
      "        [-1.9301],\n",
      "        [-2.2167],\n",
      "        [-1.8963],\n",
      "        [-1.8952],\n",
      "        [-1.9415],\n",
      "        [-2.0560],\n",
      "        [-1.7511],\n",
      "        [-2.0040],\n",
      "        [-1.6734],\n",
      "        [-1.5253]], device='cuda:0')\n",
      "Out monkey: tensor([[ 1.5413e+00],\n",
      "        [ 5.2346e-01],\n",
      "        [-1.3056e+01],\n",
      "        [ 7.3243e-01],\n",
      "        [-7.2115e+00],\n",
      "        [-1.5313e+00],\n",
      "        [ 5.4894e-01],\n",
      "        [-1.9572e+00],\n",
      "        [-9.1981e+00],\n",
      "        [-1.6105e+00],\n",
      "        [ 1.8197e+00],\n",
      "        [ 1.0866e-02],\n",
      "        [ 2.7650e+00],\n",
      "        [-2.9101e+00],\n",
      "        [ 1.2274e+00],\n",
      "        [-2.0135e+00]], device='cuda:0')\n",
      "out rodent: tensor([[ 2.1660],\n",
      "        [ 0.8548],\n",
      "        [ 1.3239],\n",
      "        [-2.1181],\n",
      "        [ 3.6405],\n",
      "        [-4.2724],\n",
      "        [-4.2107],\n",
      "        [-2.5503],\n",
      "        [ 2.1720],\n",
      "        [-1.6989],\n",
      "        [ 2.7215],\n",
      "        [-3.9442],\n",
      "        [-2.4347],\n",
      "        [-3.4085],\n",
      "        [-0.9798],\n",
      "        [ 1.8828]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.4140,  1.5335,  0.4183, -6.4058, -3.6411, -2.0769,  1.5413,  2.1660],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.2133,  0.0335, -0.0559,  0.1655,  0.1885, -0.1780, -0.1008,  0.0686],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.5269],\n",
      "        [-0.3962],\n",
      "        [-0.4451],\n",
      "        [-1.7319],\n",
      "        [-0.8730],\n",
      "        [-1.4579],\n",
      "        [-0.2795],\n",
      "        [-0.8103],\n",
      "        [-1.4134],\n",
      "        [-1.0224],\n",
      "        [-0.8004],\n",
      "        [-0.2699],\n",
      "        [-0.2364],\n",
      "        [-0.2438],\n",
      "        [-3.0286],\n",
      "        [-2.0148]], device='cuda:0')\n",
      "out bird is: tensor([[-2.0681],\n",
      "        [ 1.5987],\n",
      "        [ 0.0788],\n",
      "        [-4.0919],\n",
      "        [ 0.1040],\n",
      "        [-4.0483],\n",
      "        [ 1.4626],\n",
      "        [ 1.1654],\n",
      "        [ 2.7713],\n",
      "        [ 0.9304],\n",
      "        [ 1.1774],\n",
      "        [-0.5107],\n",
      "        [-0.0448],\n",
      "        [ 0.5526],\n",
      "        [-5.9856],\n",
      "        [ 0.1882]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.0600],\n",
      "        [-0.0180],\n",
      "        [-0.0283],\n",
      "        [-0.6626],\n",
      "        [ 0.3862],\n",
      "        [-0.3705],\n",
      "        [-0.1596],\n",
      "        [-1.1897],\n",
      "        [-0.0260],\n",
      "        [-0.1714],\n",
      "        [ 0.6776],\n",
      "        [ 0.8441],\n",
      "        [ 0.0722],\n",
      "        [ 0.8956],\n",
      "        [-0.3234],\n",
      "        [-0.4302]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -5.0281],\n",
      "        [ -5.7145],\n",
      "        [ -5.9905],\n",
      "        [ -0.2477],\n",
      "        [ -9.7476],\n",
      "        [  2.5667],\n",
      "        [ -3.8727],\n",
      "        [  6.3683],\n",
      "        [  5.4880],\n",
      "        [ -0.5438],\n",
      "        [-11.0815],\n",
      "        [ -3.9401],\n",
      "        [ -9.9756],\n",
      "        [-16.6034],\n",
      "        [ -7.2510],\n",
      "        [ -3.9097]], device='cuda:0')\n",
      "outpu_hog: tensor([[ 1.9226],\n",
      "        [-1.8129],\n",
      "        [ 4.3519],\n",
      "        [-9.2067],\n",
      "        [-0.8708],\n",
      "        [-2.7387],\n",
      "        [-3.3526],\n",
      "        [-1.7597],\n",
      "        [ 5.4759],\n",
      "        [ 5.0228],\n",
      "        [-5.7275],\n",
      "        [-2.7391],\n",
      "        [-2.0658],\n",
      "        [-6.4850],\n",
      "        [12.2737],\n",
      "        [-0.6850]], device='cuda:0')\n",
      "out leopard: tensor([[-1.8125],\n",
      "        [-1.7913],\n",
      "        [-2.0543],\n",
      "        [-1.7927],\n",
      "        [-1.7078],\n",
      "        [-1.4930],\n",
      "        [-1.9878],\n",
      "        [-1.9993],\n",
      "        [-2.1341],\n",
      "        [-1.9376],\n",
      "        [-1.6233],\n",
      "        [-2.0501],\n",
      "        [-1.5986],\n",
      "        [-1.8585],\n",
      "        [-1.7176],\n",
      "        [-1.9548]], device='cuda:0')\n",
      "Out monkey: tensor([[ -4.6764],\n",
      "        [  1.9947],\n",
      "        [  0.7918],\n",
      "        [-12.3631],\n",
      "        [  0.0752],\n",
      "        [ -3.8005],\n",
      "        [  0.9808],\n",
      "        [ -8.8695],\n",
      "        [ -3.2304],\n",
      "        [ -1.5779],\n",
      "        [  0.4253],\n",
      "        [  1.0590],\n",
      "        [  0.3825],\n",
      "        [  1.7772],\n",
      "        [-10.1520],\n",
      "        [  0.3396]], device='cuda:0')\n",
      "out rodent: tensor([[-2.1611],\n",
      "        [-0.3222],\n",
      "        [-3.0489],\n",
      "        [ 5.5715],\n",
      "        [ 0.7439],\n",
      "        [-1.6227],\n",
      "        [ 0.5299],\n",
      "        [ 3.2273],\n",
      "        [ 2.5348],\n",
      "        [-4.7466],\n",
      "        [-2.4884],\n",
      "        [-3.6081],\n",
      "        [ 0.9286],\n",
      "        [-2.4325],\n",
      "        [-6.0320],\n",
      "        [ 0.8470]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.5269, -2.0681, -0.0600, -5.0281,  1.9226, -1.8125, -4.6764, -2.1611],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1062,  0.1136, -0.0838,  0.0980,  0.2899, -0.1120, -0.0279,  0.0854],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-2.4535],\n",
      "        [-0.8545],\n",
      "        [-0.8685],\n",
      "        [-1.0885],\n",
      "        [-0.4515],\n",
      "        [-0.2838],\n",
      "        [-2.2172],\n",
      "        [-0.6840],\n",
      "        [-0.7450],\n",
      "        [-0.2561],\n",
      "        [-0.3038],\n",
      "        [-2.0416],\n",
      "        [-0.9576],\n",
      "        [-0.6813],\n",
      "        [-0.4034],\n",
      "        [-0.7842]], device='cuda:0')\n",
      "out bird is: tensor([[  1.1739],\n",
      "        [ -3.7967],\n",
      "        [  2.0864],\n",
      "        [ -1.9225],\n",
      "        [  0.5017],\n",
      "        [  0.0382],\n",
      "        [  0.7481],\n",
      "        [  2.0401],\n",
      "        [ -1.1819],\n",
      "        [ -0.2679],\n",
      "        [  1.1783],\n",
      "        [ -0.5878],\n",
      "        [ -5.0681],\n",
      "        [  0.1340],\n",
      "        [  0.5838],\n",
      "        [-10.1417]], device='cuda:0')\n",
      "Out blank is: tensor([[ 1.9551],\n",
      "        [-0.0766],\n",
      "        [-0.0477],\n",
      "        [ 0.7450],\n",
      "        [-1.0739],\n",
      "        [-0.1248],\n",
      "        [ 0.0608],\n",
      "        [-0.2694],\n",
      "        [-0.5930],\n",
      "        [ 1.1317],\n",
      "        [ 0.3525],\n",
      "        [-0.0201],\n",
      "        [-0.4073],\n",
      "        [ 0.2957],\n",
      "        [-0.0053],\n",
      "        [-0.9067]], device='cuda:0')\n",
      "Out_civet genet: tensor([[-18.1386],\n",
      "        [  7.7530],\n",
      "        [ -2.6243],\n",
      "        [  3.4650],\n",
      "        [ -3.0732],\n",
      "        [ -3.0471],\n",
      "        [ -7.6069],\n",
      "        [ -1.1689],\n",
      "        [ -5.2376],\n",
      "        [-17.5699],\n",
      "        [ -4.1496],\n",
      "        [ -0.3296],\n",
      "        [ -0.8653],\n",
      "        [ -9.2181],\n",
      "        [ -6.7707],\n",
      "        [  9.1824]], device='cuda:0')\n",
      "outpu_hog: tensor([[-2.3011],\n",
      "        [-1.1386],\n",
      "        [ 1.6684],\n",
      "        [-4.1178],\n",
      "        [-1.8006],\n",
      "        [-0.0596],\n",
      "        [-0.5511],\n",
      "        [-0.2169],\n",
      "        [-2.8245],\n",
      "        [-6.4141],\n",
      "        [ 0.5011],\n",
      "        [-1.5849],\n",
      "        [18.5429],\n",
      "        [-1.3103],\n",
      "        [-2.2036],\n",
      "        [-4.7276]], device='cuda:0')\n",
      "out leopard: tensor([[-1.9069],\n",
      "        [-1.8394],\n",
      "        [-1.8058],\n",
      "        [-1.7777],\n",
      "        [-2.1924],\n",
      "        [-2.2434],\n",
      "        [-2.0240],\n",
      "        [-2.0629],\n",
      "        [-1.9091],\n",
      "        [-1.7263],\n",
      "        [-2.0120],\n",
      "        [-1.9914],\n",
      "        [-1.6309],\n",
      "        [-1.5689],\n",
      "        [-1.6112],\n",
      "        [-2.1262]], device='cuda:0')\n",
      "Out monkey: tensor([[  0.3804],\n",
      "        [-11.3463],\n",
      "        [ -0.2162],\n",
      "        [ -8.4516],\n",
      "        [  0.4583],\n",
      "        [  0.7108],\n",
      "        [  0.2520],\n",
      "        [  1.9181],\n",
      "        [  1.2405],\n",
      "        [  2.2772],\n",
      "        [ -0.7563],\n",
      "        [ -5.5402],\n",
      "        [ -7.0962],\n",
      "        [ -0.0214],\n",
      "        [  1.5445],\n",
      "        [-11.9947]], device='cuda:0')\n",
      "out rodent: tensor([[-3.6546],\n",
      "        [-0.0768],\n",
      "        [ 1.0295],\n",
      "        [-2.7282],\n",
      "        [ 0.5537],\n",
      "        [ 0.7722],\n",
      "        [ 3.5524],\n",
      "        [ 0.9088],\n",
      "        [ 4.0649],\n",
      "        [-2.6142],\n",
      "        [-4.4631],\n",
      "        [ 4.3533],\n",
      "        [-2.9907],\n",
      "        [-2.6127],\n",
      "        [ 0.3838],\n",
      "        [ 0.7673]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([ -2.4535,   1.1739,   1.9551, -18.1386,  -2.3011,  -1.9069,   0.3804,\n",
      "         -3.6546], device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.2869, -0.1275,  0.0413,  0.1692,  0.2195, -0.2529, -0.2098,  0.0450],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.6401],\n",
      "        [-0.2891],\n",
      "        [-0.8583],\n",
      "        [-0.8582],\n",
      "        [-1.6472],\n",
      "        [-0.2643],\n",
      "        [-0.9465],\n",
      "        [-0.8302],\n",
      "        [-0.5379],\n",
      "        [-0.2478],\n",
      "        [-2.2937],\n",
      "        [-0.2441],\n",
      "        [-0.8654],\n",
      "        [-0.5889],\n",
      "        [-0.8792],\n",
      "        [-0.9487]], device='cuda:0')\n",
      "out bird is: tensor([[ 1.4450],\n",
      "        [-0.4528],\n",
      "        [-8.2930],\n",
      "        [-1.3486],\n",
      "        [ 0.5936],\n",
      "        [-0.7290],\n",
      "        [-2.0454],\n",
      "        [-0.6612],\n",
      "        [-0.5633],\n",
      "        [-0.6846],\n",
      "        [ 1.4189],\n",
      "        [ 0.3091],\n",
      "        [-0.1543],\n",
      "        [ 1.6817],\n",
      "        [-0.2733],\n",
      "        [ 1.2570]], device='cuda:0')\n",
      "Out blank is: tensor([[ 0.1094],\n",
      "        [ 0.2255],\n",
      "        [-0.0954],\n",
      "        [ 1.6557],\n",
      "        [-0.2282],\n",
      "        [ 0.0749],\n",
      "        [-0.1557],\n",
      "        [ 0.1315],\n",
      "        [-0.2303],\n",
      "        [ 0.4669],\n",
      "        [ 0.8649],\n",
      "        [ 0.3134],\n",
      "        [-0.5593],\n",
      "        [-1.7705],\n",
      "        [-0.0135],\n",
      "        [-0.2120]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -7.7185],\n",
      "        [-10.9285],\n",
      "        [  5.7218],\n",
      "        [ -3.2338],\n",
      "        [  2.3570],\n",
      "        [ -4.1602],\n",
      "        [  8.0012],\n",
      "        [ -3.7677],\n",
      "        [  1.8441],\n",
      "        [ -9.9227],\n",
      "        [-14.6089],\n",
      "        [ -7.9017],\n",
      "        [ -6.8253],\n",
      "        [ -6.4433],\n",
      "        [  2.2107],\n",
      "        [-11.9406]], device='cuda:0')\n",
      "outpu_hog: tensor([[ 2.0037],\n",
      "        [-2.9712],\n",
      "        [-5.8317],\n",
      "        [ 2.9109],\n",
      "        [-3.3419],\n",
      "        [ 0.7989],\n",
      "        [-1.8218],\n",
      "        [10.3817],\n",
      "        [-1.1951],\n",
      "        [-1.2091],\n",
      "        [-5.4262],\n",
      "        [-3.5061],\n",
      "        [-0.4829],\n",
      "        [ 3.8010],\n",
      "        [ 9.5563],\n",
      "        [-1.1402]], device='cuda:0')\n",
      "out leopard: tensor([[-1.9042],\n",
      "        [-2.0405],\n",
      "        [-2.0254],\n",
      "        [-2.0500],\n",
      "        [-2.0745],\n",
      "        [-1.8457],\n",
      "        [-2.1413],\n",
      "        [-2.0022],\n",
      "        [-1.9451],\n",
      "        [-1.9662],\n",
      "        [-1.6827],\n",
      "        [-1.9817],\n",
      "        [-1.7134],\n",
      "        [-2.3168],\n",
      "        [-1.8071],\n",
      "        [-1.3182]], device='cuda:0')\n",
      "Out monkey: tensor([[  1.3939],\n",
      "        [  2.8786],\n",
      "        [-13.8172],\n",
      "        [ -0.3449],\n",
      "        [ -4.2481],\n",
      "        [  0.9985],\n",
      "        [-10.9679],\n",
      "        [ -2.8001],\n",
      "        [ -4.9031],\n",
      "        [  0.8441],\n",
      "        [  0.9908],\n",
      "        [  0.1009],\n",
      "        [  1.0184],\n",
      "        [  1.5387],\n",
      "        [ -3.5711],\n",
      "        [ -2.8583]], device='cuda:0')\n",
      "out rodent: tensor([[-3.2112],\n",
      "        [-1.1103],\n",
      "        [ 0.3678],\n",
      "        [ 0.8117],\n",
      "        [ 2.7647],\n",
      "        [ 0.8911],\n",
      "        [ 1.7141],\n",
      "        [ 0.9710],\n",
      "        [ 4.3491],\n",
      "        [-0.6505],\n",
      "        [-1.5683],\n",
      "        [ 2.8947],\n",
      "        [ 0.6007],\n",
      "        [-5.1618],\n",
      "        [-1.9812],\n",
      "        [-6.3849]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.6401,  1.4450,  0.1094, -7.7185,  2.0037, -1.9042,  1.3939, -3.2112],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1181, -0.0842, -0.0312,  0.1089,  0.3103, -0.1760, -0.1714,  0.1405],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-3.5940],\n",
      "        [-2.2400],\n",
      "        [-0.2964],\n",
      "        [-0.3235],\n",
      "        [-1.2789],\n",
      "        [-0.4499],\n",
      "        [-1.3081],\n",
      "        [-0.2454],\n",
      "        [-0.2684],\n",
      "        [-0.7625],\n",
      "        [-0.2413],\n",
      "        [-0.5969],\n",
      "        [-2.2591],\n",
      "        [-0.2632],\n",
      "        [-1.0356],\n",
      "        [-0.2451]], device='cuda:0')\n",
      "out bird is: tensor([[-5.6524],\n",
      "        [-0.3668],\n",
      "        [-2.4482],\n",
      "        [ 2.6371],\n",
      "        [-1.9981],\n",
      "        [-0.9337],\n",
      "        [-4.2437],\n",
      "        [ 0.2457],\n",
      "        [ 0.7518],\n",
      "        [ 0.8030],\n",
      "        [ 0.0645],\n",
      "        [ 0.4680],\n",
      "        [-1.6482],\n",
      "        [-0.1728],\n",
      "        [ 1.3254],\n",
      "        [ 1.2142]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.0183],\n",
      "        [-0.5635],\n",
      "        [ 0.1194],\n",
      "        [ 0.1818],\n",
      "        [ 0.1582],\n",
      "        [-0.0139],\n",
      "        [-2.0034],\n",
      "        [-0.3289],\n",
      "        [ 0.1886],\n",
      "        [-0.2466],\n",
      "        [ 0.3514],\n",
      "        [-0.6022],\n",
      "        [ 1.0711],\n",
      "        [ 0.4982],\n",
      "        [-0.1162],\n",
      "        [ 0.0588]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -8.1525],\n",
      "        [ -4.3757],\n",
      "        [  0.1982],\n",
      "        [ -3.5175],\n",
      "        [  1.9607],\n",
      "        [ -1.5616],\n",
      "        [  9.2256],\n",
      "        [ -6.6439],\n",
      "        [ -2.5214],\n",
      "        [ -1.8690],\n",
      "        [-11.6495],\n",
      "        [  1.9396],\n",
      "        [ -1.0302],\n",
      "        [-27.1818],\n",
      "        [  2.0079],\n",
      "        [  0.0554]], device='cuda:0')\n",
      "outpu_hog: tensor([[ 3.2409],\n",
      "        [-2.5430],\n",
      "        [-1.1787],\n",
      "        [-3.8996],\n",
      "        [ 0.6182],\n",
      "        [ 1.0447],\n",
      "        [-6.5207],\n",
      "        [-1.5454],\n",
      "        [-0.2451],\n",
      "        [-6.3680],\n",
      "        [ 1.4204],\n",
      "        [ 1.8133],\n",
      "        [ 3.8541],\n",
      "        [-3.8488],\n",
      "        [ 8.6961],\n",
      "        [-1.4605]], device='cuda:0')\n",
      "out leopard: tensor([[-2.1158],\n",
      "        [-2.3921],\n",
      "        [-2.0283],\n",
      "        [-2.0305],\n",
      "        [-1.7536],\n",
      "        [-2.0207],\n",
      "        [-1.8031],\n",
      "        [-2.0684],\n",
      "        [-2.3111],\n",
      "        [-1.8567],\n",
      "        [-2.1632],\n",
      "        [-2.1024],\n",
      "        [-2.1537],\n",
      "        [-1.3912],\n",
      "        [-2.1461],\n",
      "        [-2.2862]], device='cuda:0')\n",
      "Out monkey: tensor([[-12.3655],\n",
      "        [ -0.1863],\n",
      "        [ -5.6345],\n",
      "        [  1.9281],\n",
      "        [ -4.5084],\n",
      "        [ -0.1680],\n",
      "        [ -4.9789],\n",
      "        [  0.8946],\n",
      "        [  1.5833],\n",
      "        [ -2.8678],\n",
      "        [  0.7020],\n",
      "        [ -2.9744],\n",
      "        [ -1.8701],\n",
      "        [  0.8671],\n",
      "        [ -0.1763],\n",
      "        [  0.8913]], device='cuda:0')\n",
      "out rodent: tensor([[-5.3490],\n",
      "        [ 5.2516],\n",
      "        [ 3.5650],\n",
      "        [-2.8276],\n",
      "        [ 3.7348],\n",
      "        [-4.4452],\n",
      "        [-3.0378],\n",
      "        [ 1.1016],\n",
      "        [-3.8928],\n",
      "        [ 3.5381],\n",
      "        [-2.2098],\n",
      "        [ 1.0672],\n",
      "        [-3.4795],\n",
      "        [-4.3810],\n",
      "        [-3.5106],\n",
      "        [ 2.3171]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([ -3.5940,  -5.6524,  -0.0183,  -8.1525,   3.2409,  -2.1158, -12.3655,\n",
      "         -5.3490], device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1224,  0.3055,  0.0296,  0.1072,  0.4351, -0.1779,  0.1056,  0.0052],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.2477],\n",
      "        [-0.3717],\n",
      "        [-0.3895],\n",
      "        [-0.5541],\n",
      "        [-0.3828],\n",
      "        [-0.6747],\n",
      "        [-0.6967],\n",
      "        [-1.3163],\n",
      "        [-0.4624],\n",
      "        [-0.8760],\n",
      "        [-0.6808],\n",
      "        [-1.2802],\n",
      "        [-1.3424],\n",
      "        [-0.3159],\n",
      "        [-0.2242],\n",
      "        [-2.7805]], device='cuda:0')\n",
      "out bird is: tensor([[ 0.5318],\n",
      "        [ 1.0600],\n",
      "        [ 0.5600],\n",
      "        [ 0.1552],\n",
      "        [ 0.8985],\n",
      "        [-2.4329],\n",
      "        [-6.1642],\n",
      "        [-7.1931],\n",
      "        [-0.0872],\n",
      "        [ 0.9824],\n",
      "        [ 1.8213],\n",
      "        [ 2.5451],\n",
      "        [-5.1936],\n",
      "        [ 0.1902],\n",
      "        [ 0.9920],\n",
      "        [-0.5924]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.0083],\n",
      "        [ 0.8928],\n",
      "        [ 0.6787],\n",
      "        [ 0.0250],\n",
      "        [-0.8824],\n",
      "        [-0.8049],\n",
      "        [-0.3308],\n",
      "        [-0.0225],\n",
      "        [ 0.2172],\n",
      "        [-0.2073],\n",
      "        [ 0.0499],\n",
      "        [ 0.8706],\n",
      "        [-0.1615],\n",
      "        [ 0.6262],\n",
      "        [-0.3118],\n",
      "        [ 1.5358]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -9.2141],\n",
      "        [-14.6610],\n",
      "        [ -6.6262],\n",
      "        [ -1.8211],\n",
      "        [ -5.1290],\n",
      "        [  9.3950],\n",
      "        [  1.7809],\n",
      "        [ -1.6241],\n",
      "        [ -6.4916],\n",
      "        [ -0.1893],\n",
      "        [ -0.7129],\n",
      "        [ -3.2827],\n",
      "        [ -7.7310],\n",
      "        [ -6.6768],\n",
      "        [ -2.8235],\n",
      "        [-11.2819]], device='cuda:0')\n",
      "outpu_hog: tensor([[-1.8406],\n",
      "        [-6.4160],\n",
      "        [ 1.3873],\n",
      "        [ 1.7648],\n",
      "        [ 5.1235],\n",
      "        [-4.4428],\n",
      "        [-8.1033],\n",
      "        [ 7.5845],\n",
      "        [-0.4344],\n",
      "        [ 7.0269],\n",
      "        [ 2.3378],\n",
      "        [-0.1191],\n",
      "        [-0.9741],\n",
      "        [-1.9046],\n",
      "        [-3.9736],\n",
      "        [-7.5790]], device='cuda:0')\n",
      "out leopard: tensor([[-2.2261],\n",
      "        [-1.7702],\n",
      "        [-1.5477],\n",
      "        [-2.1562],\n",
      "        [-2.1555],\n",
      "        [-1.7093],\n",
      "        [-1.7614],\n",
      "        [-2.0983],\n",
      "        [-2.2297],\n",
      "        [-2.4646],\n",
      "        [-2.0847],\n",
      "        [-2.1800],\n",
      "        [-1.6118],\n",
      "        [-1.6846],\n",
      "        [-2.2630],\n",
      "        [-2.2325]], device='cuda:0')\n",
      "Out monkey: tensor([[  1.5884],\n",
      "        [  0.7878],\n",
      "        [  0.4659],\n",
      "        [ -0.2152],\n",
      "        [  1.1069],\n",
      "        [ -6.8808],\n",
      "        [-14.6166],\n",
      "        [-11.3251],\n",
      "        [  0.3175],\n",
      "        [ -3.9442],\n",
      "        [ -1.8737],\n",
      "        [  0.9328],\n",
      "        [ -5.2640],\n",
      "        [  1.5853],\n",
      "        [  0.6299],\n",
      "        [  1.2526]], device='cuda:0')\n",
      "out rodent: tensor([[-3.0502],\n",
      "        [-3.7793],\n",
      "        [-0.0407],\n",
      "        [-1.6720],\n",
      "        [-0.3769],\n",
      "        [ 2.5667],\n",
      "        [ 5.5537],\n",
      "        [ 3.5946],\n",
      "        [-0.8555],\n",
      "        [ 2.8793],\n",
      "        [-3.4111],\n",
      "        [ 1.5365],\n",
      "        [-5.2556],\n",
      "        [ 0.9997],\n",
      "        [ 1.5893],\n",
      "        [-5.7971]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-2.4769e-01,  5.3182e-01, -8.3163e-03, -9.2141e+00, -1.8406e+00,\n",
      "        -2.2261e+00,  1.5884e+00, -3.0502e+00], device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1947, -0.0444, -0.0643,  0.1653,  0.2086, -0.2032, -0.1429,  0.0985],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.5308],\n",
      "        [-0.2854],\n",
      "        [-0.8031],\n",
      "        [-0.6071],\n",
      "        [-0.2539],\n",
      "        [-2.1340],\n",
      "        [-0.3989],\n",
      "        [-0.8203],\n",
      "        [-0.5728],\n",
      "        [-0.2417],\n",
      "        [-0.5656],\n",
      "        [-2.3096],\n",
      "        [-2.3057],\n",
      "        [-0.9727],\n",
      "        [-1.1743],\n",
      "        [-0.2699]], device='cuda:0')\n",
      "out bird is: tensor([[-1.1330],\n",
      "        [ 0.1497],\n",
      "        [-4.1081],\n",
      "        [ 2.3819],\n",
      "        [ 0.1936],\n",
      "        [-2.6655],\n",
      "        [ 0.3340],\n",
      "        [ 0.3211],\n",
      "        [-6.2411],\n",
      "        [ 0.2764],\n",
      "        [-1.2040],\n",
      "        [-2.7424],\n",
      "        [-0.1880],\n",
      "        [ 1.0669],\n",
      "        [ 2.9926],\n",
      "        [ 0.4145]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.1680],\n",
      "        [ 0.7978],\n",
      "        [ 0.6126],\n",
      "        [-0.1678],\n",
      "        [-0.3231],\n",
      "        [ 0.0591],\n",
      "        [-0.7339],\n",
      "        [ 0.1199],\n",
      "        [-0.4152],\n",
      "        [ 0.0555],\n",
      "        [ 0.3217],\n",
      "        [ 0.9042],\n",
      "        [ 1.0602],\n",
      "        [-0.4496],\n",
      "        [-0.4502],\n",
      "        [ 0.2821]], device='cuda:0')\n",
      "Out_civet genet: tensor([[  1.0799],\n",
      "        [-10.9916],\n",
      "        [ -0.4582],\n",
      "        [ -8.0739],\n",
      "        [ -9.2091],\n",
      "        [ -6.1625],\n",
      "        [  0.1189],\n",
      "        [ -3.2410],\n",
      "        [  3.0638],\n",
      "        [-18.8337],\n",
      "        [ -1.1113],\n",
      "        [ -7.4778],\n",
      "        [ -7.5231],\n",
      "        [  5.4502],\n",
      "        [  5.6280],\n",
      "        [ -7.5706]], device='cuda:0')\n",
      "outpu_hog: tensor([[-5.9710],\n",
      "        [ 3.9848],\n",
      "        [-0.7836],\n",
      "        [-1.6635],\n",
      "        [-3.6708],\n",
      "        [ 3.5279],\n",
      "        [-1.9215],\n",
      "        [ 1.5228],\n",
      "        [-0.7317],\n",
      "        [-4.3857],\n",
      "        [-0.0515],\n",
      "        [ 1.5428],\n",
      "        [ 1.1614],\n",
      "        [-1.2320],\n",
      "        [ 6.1561],\n",
      "        [-5.7253]], device='cuda:0')\n",
      "out leopard: tensor([[-1.9523],\n",
      "        [-1.6879],\n",
      "        [-2.1168],\n",
      "        [-1.6044],\n",
      "        [-1.7116],\n",
      "        [-2.1296],\n",
      "        [-2.0982],\n",
      "        [-2.0377],\n",
      "        [-2.0761],\n",
      "        [-1.7160],\n",
      "        [-2.1095],\n",
      "        [-2.1151],\n",
      "        [-2.0230],\n",
      "        [-1.9296],\n",
      "        [-1.7472],\n",
      "        [-1.8440]], device='cuda:0')\n",
      "Out monkey: tensor([[-6.3628],\n",
      "        [ 0.7716],\n",
      "        [-8.1462],\n",
      "        [-0.2080],\n",
      "        [ 0.3954],\n",
      "        [-3.7060],\n",
      "        [-0.4862],\n",
      "        [ 2.6924],\n",
      "        [-5.9432],\n",
      "        [ 2.0962],\n",
      "        [-4.5529],\n",
      "        [-5.8097],\n",
      "        [ 0.7745],\n",
      "        [-1.6669],\n",
      "        [-3.2858],\n",
      "        [ 0.6125]], device='cuda:0')\n",
      "out rodent: tensor([[ 0.2117],\n",
      "        [-3.1676],\n",
      "        [ 1.8388],\n",
      "        [ 1.3510],\n",
      "        [ 0.8197],\n",
      "        [-3.5525],\n",
      "        [ 1.5013],\n",
      "        [ 0.4145],\n",
      "        [ 3.5972],\n",
      "        [-0.7652],\n",
      "        [ 5.4547],\n",
      "        [-6.9064],\n",
      "        [-3.4274],\n",
      "        [-0.4316],\n",
      "        [ 3.1205],\n",
      "        [-6.3413]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.5308, -1.1330, -0.1680,  1.0799, -5.9710, -1.9523, -6.3628,  0.2117],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1155,  0.0367, -0.1266,  0.1890,  0.1628, -0.1397, -0.1337,  0.1141],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.6933],\n",
      "        [-0.8997],\n",
      "        [-0.8466],\n",
      "        [-0.3401],\n",
      "        [-2.4956],\n",
      "        [-0.5854],\n",
      "        [-1.0114],\n",
      "        [-0.2660],\n",
      "        [-0.8421],\n",
      "        [-1.2155],\n",
      "        [-0.3677],\n",
      "        [-0.3336],\n",
      "        [-1.9514],\n",
      "        [-0.6670],\n",
      "        [-0.4078],\n",
      "        [-0.3896]], device='cuda:0')\n",
      "out bird is: tensor([[-1.2296],\n",
      "        [-1.9950],\n",
      "        [ 2.0723],\n",
      "        [ 0.5915],\n",
      "        [-8.9027],\n",
      "        [-0.6932],\n",
      "        [ 0.4426],\n",
      "        [-0.9419],\n",
      "        [-0.4990],\n",
      "        [ 0.5894],\n",
      "        [ 0.8334],\n",
      "        [ 3.1834],\n",
      "        [-0.0934],\n",
      "        [-1.5664],\n",
      "        [ 0.2098],\n",
      "        [-1.3798]], device='cuda:0')\n",
      "Out blank is: tensor([[-0.3774],\n",
      "        [-0.2676],\n",
      "        [ 0.1476],\n",
      "        [ 1.3860],\n",
      "        [-0.1283],\n",
      "        [ 0.0578],\n",
      "        [ 0.2187],\n",
      "        [ 0.4102],\n",
      "        [-0.1474],\n",
      "        [-0.9074],\n",
      "        [-0.5443],\n",
      "        [ 0.5157],\n",
      "        [ 1.4499],\n",
      "        [ 0.3587],\n",
      "        [ 0.0529],\n",
      "        [ 0.0532]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -8.2140],\n",
      "        [  3.4094],\n",
      "        [ -8.6117],\n",
      "        [-15.5384],\n",
      "        [ -7.7342],\n",
      "        [  0.1475],\n",
      "        [  3.5266],\n",
      "        [ -9.4490],\n",
      "        [  1.8572],\n",
      "        [ -4.9126],\n",
      "        [-13.8848],\n",
      "        [  1.8135],\n",
      "        [ -2.3031],\n",
      "        [  4.4549],\n",
      "        [ -7.1836],\n",
      "        [  2.6130]], device='cuda:0')\n",
      "outpu_hog: tensor([[-0.4469],\n",
      "        [-8.7632],\n",
      "        [ 0.7186],\n",
      "        [-4.7135],\n",
      "        [ 1.3013],\n",
      "        [ 2.9815],\n",
      "        [ 6.6995],\n",
      "        [-2.2616],\n",
      "        [-1.9617],\n",
      "        [ 8.9888],\n",
      "        [-3.7324],\n",
      "        [-1.4983],\n",
      "        [ 3.5576],\n",
      "        [ 3.7476],\n",
      "        [-0.6745],\n",
      "        [-6.7397]], device='cuda:0')\n",
      "out leopard: tensor([[-2.5864],\n",
      "        [-1.7524],\n",
      "        [-1.8290],\n",
      "        [-1.6954],\n",
      "        [-2.0253],\n",
      "        [-2.1218],\n",
      "        [-1.9388],\n",
      "        [-1.8703],\n",
      "        [-1.9617],\n",
      "        [-2.1100],\n",
      "        [-1.6638],\n",
      "        [-2.0740],\n",
      "        [-1.8981],\n",
      "        [-2.0602],\n",
      "        [-1.7790],\n",
      "        [-2.2037]], device='cuda:0')\n",
      "Out monkey: tensor([[ -2.2424],\n",
      "        [  0.3081],\n",
      "        [  2.6957],\n",
      "        [  0.5231],\n",
      "        [ -3.1455],\n",
      "        [  0.0868],\n",
      "        [ -6.9996],\n",
      "        [  0.9425],\n",
      "        [  1.1250],\n",
      "        [ -1.7408],\n",
      "        [  1.5450],\n",
      "        [ -0.0237],\n",
      "        [ -3.7574],\n",
      "        [ -6.3031],\n",
      "        [  0.0763],\n",
      "        [-13.7320]], device='cuda:0')\n",
      "out rodent: tensor([[-1.5875],\n",
      "        [ 5.4870],\n",
      "        [-2.5323],\n",
      "        [-4.7371],\n",
      "        [-5.2175],\n",
      "        [-3.3727],\n",
      "        [ 0.2435],\n",
      "        [-2.4180],\n",
      "        [-0.3228],\n",
      "        [-2.8740],\n",
      "        [-2.3503],\n",
      "        [ 1.9483],\n",
      "        [ 2.4661],\n",
      "        [ 0.6955],\n",
      "        [ 2.6481],\n",
      "        [ 5.1053]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-0.6933, -1.2296, -0.3774, -8.2140, -0.4469, -2.5864, -2.2424, -1.5875],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1641,  0.0459, -0.0792,  0.1184,  0.2287, -0.1551, -0.0550,  0.0648],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2], Step [10/824], Loss: 2.1076\n",
      "out Antelope is: tensor([[-0.3883],\n",
      "        [-0.2562],\n",
      "        [-1.6913],\n",
      "        [-0.7469],\n",
      "        [-1.0724],\n",
      "        [-1.4169],\n",
      "        [-2.3499],\n",
      "        [-0.2533],\n",
      "        [-0.2925],\n",
      "        [-0.9159],\n",
      "        [-0.7342],\n",
      "        [-0.9022],\n",
      "        [-0.2634],\n",
      "        [-1.1419],\n",
      "        [-0.3840],\n",
      "        [-0.7810]], device='cuda:0')\n",
      "out bird is: tensor([[ -1.1229],\n",
      "        [ -0.3828],\n",
      "        [  0.7222],\n",
      "        [ -2.8434],\n",
      "        [  1.0231],\n",
      "        [-11.0749],\n",
      "        [ -0.2108],\n",
      "        [  0.4250],\n",
      "        [ -0.6425],\n",
      "        [  1.4803],\n",
      "        [  2.2147],\n",
      "        [ -2.2815],\n",
      "        [  0.6198],\n",
      "        [ -0.1965],\n",
      "        [  1.7692],\n",
      "        [ -2.6249]], device='cuda:0')\n",
      "Out blank is: tensor([[-1.6048],\n",
      "        [-0.1589],\n",
      "        [ 0.8265],\n",
      "        [ 0.0487],\n",
      "        [-0.2280],\n",
      "        [-0.2887],\n",
      "        [-0.0505],\n",
      "        [ 1.7662],\n",
      "        [-0.0730],\n",
      "        [-0.0810],\n",
      "        [-0.0320],\n",
      "        [ 0.4928],\n",
      "        [ 0.0307],\n",
      "        [ 1.5235],\n",
      "        [ 0.0370],\n",
      "        [-0.9848]], device='cuda:0')\n",
      "Out_civet genet: tensor([[ -1.0379],\n",
      "        [ -8.1137],\n",
      "        [-11.4419],\n",
      "        [  1.1659],\n",
      "        [ -9.7896],\n",
      "        [  9.5391],\n",
      "        [ -4.1609],\n",
      "        [ -9.7799],\n",
      "        [  2.1536],\n",
      "        [ -4.4066],\n",
      "        [ -5.9038],\n",
      "        [ -6.2750],\n",
      "        [ -7.0814],\n",
      "        [ -0.9290],\n",
      "        [ -4.4731],\n",
      "        [  1.8773]], device='cuda:0')\n",
      "outpu_hog: tensor([[-3.0750e+00],\n",
      "        [-4.0768e+00],\n",
      "        [-4.3302e+00],\n",
      "        [-7.7985e+00],\n",
      "        [-6.2287e+00],\n",
      "        [ 5.5105e+00],\n",
      "        [ 1.6681e+00],\n",
      "        [-3.0758e+00],\n",
      "        [-6.0936e-01],\n",
      "        [ 1.5442e+00],\n",
      "        [-2.0577e-01],\n",
      "        [ 5.4848e+00],\n",
      "        [-2.4136e+00],\n",
      "        [ 2.2176e+00],\n",
      "        [-3.7696e-03],\n",
      "        [ 1.1320e+01]], device='cuda:0')\n",
      "out leopard: tensor([[-1.7762],\n",
      "        [-1.9077],\n",
      "        [-2.0959],\n",
      "        [-1.8356],\n",
      "        [-1.7979],\n",
      "        [-1.5511],\n",
      "        [-2.0368],\n",
      "        [-1.7253],\n",
      "        [-2.0148],\n",
      "        [-1.9258],\n",
      "        [-1.6201],\n",
      "        [-1.8304],\n",
      "        [-2.3390],\n",
      "        [-1.9969],\n",
      "        [-1.9636],\n",
      "        [-1.9511]], device='cuda:0')\n",
      "Out monkey: tensor([[-11.6117],\n",
      "        [  2.4562],\n",
      "        [  0.2991],\n",
      "        [ -5.8852],\n",
      "        [  1.7249],\n",
      "        [-12.4444],\n",
      "        [  0.3202],\n",
      "        [  0.3039],\n",
      "        [  0.7593],\n",
      "        [  0.5341],\n",
      "        [  0.3955],\n",
      "        [ -6.2230],\n",
      "        [  0.7402],\n",
      "        [  0.8236],\n",
      "        [  1.1078],\n",
      "        [ -5.1106]], device='cuda:0')\n",
      "out rodent: tensor([[ 6.7164],\n",
      "        [ 0.8490],\n",
      "        [-2.9230],\n",
      "        [ 0.2373],\n",
      "        [-1.9748],\n",
      "        [ 0.5648],\n",
      "        [ 0.6418],\n",
      "        [-2.8387],\n",
      "        [-4.8161],\n",
      "        [-5.0606],\n",
      "        [ 2.3629],\n",
      "        [-1.4408],\n",
      "        [-2.9661],\n",
      "        [-2.1293],\n",
      "        [ 2.0086],\n",
      "        [-0.4943]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([ -0.3883,  -1.1229,  -1.6048,  -1.0379,  -3.0750,  -1.7762, -11.6117,\n",
      "          6.7164], device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.0472,  0.0465, -0.1662,  0.1973,  0.3216, -0.1091, -0.2263,  0.2488],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-0.7244],\n",
      "        [-1.9121],\n",
      "        [-0.6499],\n",
      "        [-0.6432],\n",
      "        [-0.4886],\n",
      "        [-0.1979],\n",
      "        [-0.3131],\n",
      "        [-1.5769],\n",
      "        [-1.4918],\n",
      "        [-0.2011],\n",
      "        [-0.3475],\n",
      "        [-1.5973],\n",
      "        [-1.4410],\n",
      "        [-0.3478],\n",
      "        [-0.5519],\n",
      "        [-2.1936]], device='cuda:0')\n",
      "out bird is: tensor([[-2.0823],\n",
      "        [ 0.1002],\n",
      "        [-3.8446],\n",
      "        [ 2.1451],\n",
      "        [ 0.1682],\n",
      "        [-0.5258],\n",
      "        [-0.6204],\n",
      "        [ 0.2885],\n",
      "        [ 2.0922],\n",
      "        [-0.3577],\n",
      "        [ 1.0628],\n",
      "        [-2.0358],\n",
      "        [-1.3296],\n",
      "        [ 1.2994],\n",
      "        [ 2.2511],\n",
      "        [-5.2296]], device='cuda:0')\n",
      "Out blank is: tensor([[ 0.2269],\n",
      "        [ 0.0420],\n",
      "        [-0.6673],\n",
      "        [ 0.2147],\n",
      "        [ 0.8156],\n",
      "        [-1.8511],\n",
      "        [ 0.3488],\n",
      "        [ 1.1880],\n",
      "        [-0.0203],\n",
      "        [ 0.3623],\n",
      "        [ 0.6862],\n",
      "        [ 0.9816],\n",
      "        [-0.4176],\n",
      "        [-0.0048],\n",
      "        [ 0.0188],\n",
      "        [-0.1373]], device='cuda:0')\n",
      "Out_civet genet: tensor([[  0.7320],\n",
      "        [ -7.4985],\n",
      "        [ -0.2863],\n",
      "        [  0.9229],\n",
      "        [-11.8759],\n",
      "        [  4.8312],\n",
      "        [ -4.7371],\n",
      "        [ -8.6347],\n",
      "        [  5.7541],\n",
      "        [ -7.0267],\n",
      "        [-26.5359],\n",
      "        [  3.2646],\n",
      "        [ -7.0357],\n",
      "        [-10.6289],\n",
      "        [ -4.4313],\n",
      "        [  2.3520]], device='cuda:0')\n",
      "outpu_hog: tensor([[-7.5387],\n",
      "        [ 7.8168],\n",
      "        [13.5695],\n",
      "        [ 0.6200],\n",
      "        [-7.2434],\n",
      "        [-2.5372],\n",
      "        [-2.7588],\n",
      "        [-0.3175],\n",
      "        [ 1.1506],\n",
      "        [ 6.6079],\n",
      "        [-4.2701],\n",
      "        [-4.0806],\n",
      "        [ 1.1063],\n",
      "        [-0.2061],\n",
      "        [-2.9994],\n",
      "        [-1.6153]], device='cuda:0')\n",
      "out leopard: tensor([[-2.0004],\n",
      "        [-1.6888],\n",
      "        [-1.8918],\n",
      "        [-2.1613],\n",
      "        [-1.8779],\n",
      "        [-1.4444],\n",
      "        [-2.0868],\n",
      "        [-1.8597],\n",
      "        [-1.9738],\n",
      "        [-1.7514],\n",
      "        [-1.7893],\n",
      "        [-1.5456],\n",
      "        [-1.9907],\n",
      "        [-2.0106],\n",
      "        [-2.1625],\n",
      "        [-1.7986]], device='cuda:0')\n",
      "Out monkey: tensor([[-13.9781],\n",
      "        [  1.2939],\n",
      "        [ -7.3839],\n",
      "        [  0.2272],\n",
      "        [  1.8201],\n",
      "        [ -6.5885],\n",
      "        [  0.7081],\n",
      "        [  0.4425],\n",
      "        [ -5.3702],\n",
      "        [  0.6012],\n",
      "        [  0.8233],\n",
      "        [ -1.4348],\n",
      "        [ -1.9274],\n",
      "        [  1.8696],\n",
      "        [  0.2329],\n",
      "        [ -6.0887]], device='cuda:0')\n",
      "out rodent: tensor([[ 2.4510],\n",
      "        [-5.1853],\n",
      "        [-1.2793],\n",
      "        [ 2.3833],\n",
      "        [-6.4545],\n",
      "        [ 0.9972],\n",
      "        [ 1.9735],\n",
      "        [-1.5635],\n",
      "        [ 1.5124],\n",
      "        [-4.6335],\n",
      "        [-5.4009],\n",
      "        [ 2.3864],\n",
      "        [-4.7085],\n",
      "        [ 1.1550],\n",
      "        [ 3.5863],\n",
      "        [ 4.3132]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([ -0.7244,  -2.0823,   0.2269,   0.7320,  -7.5387,  -2.0004, -13.9781,\n",
      "          2.4510], device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.1340,  0.0390, -0.1789,  0.2711,  0.1419, -0.1197, -0.1851,  0.1237],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "out Antelope is: tensor([[-2.2620],\n",
      "        [-1.1761],\n",
      "        [-0.4940],\n",
      "        [-0.3339],\n",
      "        [-0.9602],\n",
      "        [-0.4266],\n",
      "        [-1.5094],\n",
      "        [-0.2794],\n",
      "        [-1.1998],\n",
      "        [-0.7890],\n",
      "        [-0.7747],\n",
      "        [-0.3929],\n",
      "        [-0.5112],\n",
      "        [-0.5436],\n",
      "        [-0.5613],\n",
      "        [-2.6996]], device='cuda:0')\n",
      "out bird is: tensor([[-5.8943],\n",
      "        [-8.4524],\n",
      "        [ 0.6671],\n",
      "        [-0.0327],\n",
      "        [ 2.1944],\n",
      "        [-0.3349],\n",
      "        [ 0.2642],\n",
      "        [ 0.3222],\n",
      "        [-1.3020],\n",
      "        [-1.3345],\n",
      "        [ 1.4914],\n",
      "        [ 0.2175],\n",
      "        [ 0.6756],\n",
      "        [ 1.5226],\n",
      "        [ 0.3569],\n",
      "        [ 1.1776]], device='cuda:0')\n",
      "Out blank is: tensor([[ 0.0165],\n",
      "        [-0.9077],\n",
      "        [ 0.5382],\n",
      "        [ 0.2495],\n",
      "        [-0.8888],\n",
      "        [ 1.4280],\n",
      "        [ 0.0911],\n",
      "        [-0.0928],\n",
      "        [-0.7069],\n",
      "        [-0.2341],\n",
      "        [ 0.0820],\n",
      "        [ 1.0708],\n",
      "        [ 0.0164],\n",
      "        [ 0.7081],\n",
      "        [ 0.5366],\n",
      "        [-0.1338]], device='cuda:0')\n",
      "Out_civet genet: tensor([[  3.0539],\n",
      "        [  8.0560],\n",
      "        [ -6.3915],\n",
      "        [ -6.2571],\n",
      "        [  4.1430],\n",
      "        [ -3.8016],\n",
      "        [ -4.8168],\n",
      "        [ -7.8399],\n",
      "        [  1.3581],\n",
      "        [ -0.0138],\n",
      "        [  4.7659],\n",
      "        [ -9.8192],\n",
      "        [-10.2160],\n",
      "        [ -7.6856],\n",
      "        [-12.0190],\n",
      "        [-10.7470]], device='cuda:0')\n",
      "outpu_hog: tensor([[-2.6162],\n",
      "        [-3.1820],\n",
      "        [-2.5368],\n",
      "        [-1.0519],\n",
      "        [-1.3548],\n",
      "        [ 5.4878],\n",
      "        [-0.3868],\n",
      "        [ 2.1312],\n",
      "        [-0.2158],\n",
      "        [ 7.0082],\n",
      "        [-6.8305],\n",
      "        [-3.2266],\n",
      "        [ 7.8101],\n",
      "        [-1.2513],\n",
      "        [-6.8565],\n",
      "        [ 0.4392]], device='cuda:0')\n",
      "out leopard: tensor([[-1.9764],\n",
      "        [-2.1895],\n",
      "        [-1.9906],\n",
      "        [-1.8834],\n",
      "        [-1.9696],\n",
      "        [-1.8848],\n",
      "        [-2.1471],\n",
      "        [-2.2292],\n",
      "        [-1.8286],\n",
      "        [-1.9736],\n",
      "        [-2.0702],\n",
      "        [-2.0415],\n",
      "        [-2.0244],\n",
      "        [-2.1186],\n",
      "        [-1.4914],\n",
      "        [-1.8765]], device='cuda:0')\n",
      "Out monkey: tensor([[ -8.2207],\n",
      "        [ -4.7545],\n",
      "        [  1.5211],\n",
      "        [  1.3879],\n",
      "        [ -8.5709],\n",
      "        [  1.7323],\n",
      "        [  0.4924],\n",
      "        [  1.3319],\n",
      "        [ -7.2927],\n",
      "        [ -2.5868],\n",
      "        [-11.0731],\n",
      "        [  0.3456],\n",
      "        [  1.5753],\n",
      "        [  0.3461],\n",
      "        [  1.4503],\n",
      "        [ -3.0710]], device='cuda:0')\n",
      "out rodent: tensor([[ 8.8389],\n",
      "        [ 2.9270],\n",
      "        [-1.5127],\n",
      "        [ 2.7132],\n",
      "        [-1.2565],\n",
      "        [-1.6313],\n",
      "        [-1.0685],\n",
      "        [-0.7152],\n",
      "        [-3.3053],\n",
      "        [-4.4922],\n",
      "        [ 0.8009],\n",
      "        [-2.7870],\n",
      "        [-3.5674],\n",
      "        [ 0.0332],\n",
      "        [-0.2590],\n",
      "        [-2.6645]], device='cuda:0')\n",
      "Mega X is:\n",
      "tensor([-2.2620, -5.8943,  0.0165,  3.0539, -2.6162, -1.9764, -8.2207,  8.8389],\n",
      "       device='cuda:0')\n",
      "Outputs in the training loop:\n",
      "tensor([ 0.0462,  0.0279, -0.1113,  0.1700,  0.1655, -0.1152, -0.1645,  0.1097],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# # Training step\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     avg_train_loss, tracking_loss \u001b[38;5;241m=\u001b[39m train(model, \n\u001b[0;32m     11\u001b[0m                                      train_loader, \n\u001b[0;32m     12\u001b[0m                                      criterion, \n\u001b[0;32m     13\u001b[0m                                      optimizer, \n\u001b[0;32m     14\u001b[0m                                      epoch, config, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     15\u001b[0m     tracking_loss_all\u001b[38;5;241m.\u001b[39mextend(tracking_loss)  \u001b[38;5;66;03m# Append to global list\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)  \u001b[38;5;66;03m# Store avg training loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\Documents\\Georgia Tech\\Deep Learning\\Final Project\\Github Code\\wildlife\\ensemble_classifier\\train.py:51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, epoch, config, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     53\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[22], line 112\u001b[0m, in \u001b[0;36mEnsembleClassifier.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    110\u001b[0m out_blank \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_blank(X))\n\u001b[0;32m    111\u001b[0m out_civet_genet \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_civet_genet(X))\n\u001b[1;32m--> 112\u001b[0m out_hog \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_hog(X))\n\u001b[0;32m    113\u001b[0m out_leopard \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_leopard(X))\n\u001b[0;32m    114\u001b[0m out_monkey_prosimian \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_monkey_prosimian(X))\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torchvision\\models\\resnet.py:280\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_counter = 0\n",
    "tracking_loss_all = []\n",
    "train_losses = []  # To store average training loss per epoch\n",
    "val_losses = []    # To store validation loss per epoch\n",
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    # # Training step\n",
    "    avg_train_loss, tracking_loss = train(model, \n",
    "                                     train_loader, \n",
    "                                     criterion, \n",
    "                                     optimizer, \n",
    "                                     epoch, config, device=device)\n",
    "    tracking_loss_all.extend(tracking_loss)  # Append to global list\n",
    "    train_losses.append(avg_train_loss)  # Store avg training loss\n",
    "    print(f\"Epoch {epoch+1}/{config['train']['epochs']} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation step\n",
    "    eval_metrics = evaluate(model, val_loader, criterion, config, epoch= epoch+1, device=device)\n",
    "    val_losses.append(eval_metrics[\"loss\"])  # Store validation loss\n",
    "    print(f\"Epoch {epoch+1}/{config['train']['epochs']} - Eval Loss: {eval_metrics['loss']:.4f}, Eval Acc: {eval_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "wandb.log({\"duration\": duration})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done logging or you want to run the experiment again, finish with the block below. But if you think you might want to submit this run to the competition, don't finish logging until the end once you've added the competition score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ¨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore Experiment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to explore and potentially submit your results \n",
    "explore = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Loss During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # Convert tracking_loss to a pandas Series for convenient rolling average\n",
    "    tracking_loss_series = pd.Series(tracking_loss_all)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    tracking_loss_series.plot(alpha=0.2, label=\"Batch Loss\")\n",
    "    tracking_loss_series.rolling(center=True, min_periods=1, window=10).mean().plot(\n",
    "        label=\"Loss (Moving Avg)\", linewidth=2\n",
    "    )\n",
    "    plt.xlabel(\"(Epoch, Batch)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Batch Loss During Training\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Labels from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    print(\"True labels (training):\")\n",
    "    print(y_train.idxmax(axis=1).value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True and Predicated Labels from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # Extract predictions and true labels from eval_metrics\n",
    "    all_preds = eval_metrics[\"all_preds\"]\n",
    "    all_labels = eval_metrics[\"all_labels\"]\n",
    "\n",
    "    # Convert all_preds to DataFrame and map to class names\n",
    "    preds_df = pd.DataFrame(all_preds, columns=[\"predicted_class\"])\n",
    "    preds_df[\"predicted_label\"] = preds_df[\"predicted_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Convert all_labels to DataFrame and map to class names\n",
    "    labels_df = pd.DataFrame(all_labels, columns=[\"true_class\"])\n",
    "    labels_df[\"true_label\"] = labels_df[\"true_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Combine predictions and true labels for analysis\n",
    "    results_df = pd.concat([preds_df, labels_df], axis=1)\n",
    "\n",
    "    # Display value counts for predicted and true labels\n",
    "    print(\"Predicted labels (eval):\")\n",
    "    print(results_df[\"predicted_label\"].value_counts())\n",
    "\n",
    "    print(\"\\nTrue labels (eval):\")\n",
    "    print(results_df[\"true_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:    \n",
    "    per_class_accuracy = results_df.groupby(\"true_label\").apply(\n",
    "        lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).mean(), \n",
    "    )\n",
    "    print(\"Per-Class Accuracy:\")\n",
    "    print(per_class_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "    eval_true = pd.Series(all_labels).apply(lambda x: species_labels[x])\n",
    "    eval_predictions = pd.Series(all_preds).apply(lambda x: species_labels[x])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(\n",
    "        eval_true,\n",
    "        eval_predictions,\n",
    "        ax=ax,\n",
    "        xticks_rotation=90,\n",
    "        colorbar=True,\n",
    "        normalize='true'\n",
    "    )\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Datatloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    set_seeds(config[\"experiment\"][\"seed\"])\n",
    "    test_dataset = ImagesDataset(\n",
    "        test_features, \n",
    "        transform=val_transforms, \n",
    "        device=device)\n",
    "\n",
    "    if(device==\"cuda\"):\n",
    "        pin=False\n",
    "    else:\n",
    "        pin=True\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config[\"train\"][\"batch_size\"], \n",
    "        shuffle=False, pin_memory=pin)\n",
    "    \n",
    "    print(f\"Test set: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    test_preds_collector = []\n",
    "\n",
    "    # put the model in eval mode so we don't update any parameters\n",
    "    model.eval()\n",
    "\n",
    "    # we aren't updating our weights so no need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_n, batch in enumerate(test_loader):\n",
    "            # run the forward step\n",
    "            images = batch[\"image\"].to(device)\n",
    "            logits = model(images)\n",
    "\n",
    "            # apply softmax so that model outputs are in range [0,1]\n",
    "            preds = F.softmax(logits, dim=1)\n",
    "\n",
    "            # store this batch's predictions in df\n",
    "            # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n",
    "            preds_df = pd.DataFrame(\n",
    "                preds.cpu().numpy(),\n",
    "                index=batch[\"image_id\"],\n",
    "                columns=species_labels,\n",
    "            )\n",
    "            test_preds_collector.append(preds_df)\n",
    "\n",
    "    submission_df = pd.concat(test_preds_collector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your submission. Update submission_number.\n",
    "\n",
    "Make sure your directory is properly set up, as both `/data` and `/results` are ignored by the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    submission_number=14\n",
    "    submission_df.index.name = 'id'\n",
    "    submission_df = submission_df.round(6)\n",
    "    submission_format_path = \"../data/givens/submission_format.csv\"\n",
    "    submission_format = pd.read_csv(submission_format_path, index_col=\"id\")\n",
    "\n",
    "\n",
    "    assert all(submission_df.index == submission_format.index)\n",
    "    assert all(submission_df.columns == submission_format.columns)\n",
    "\n",
    "    # Save submission_df for further use\n",
    "    submission_df_path = f\"../results/submissions/submission{submission_number}.csv\"\n",
    "    submission_df.to_csv(submission_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you submit update the submission score for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "     # âœ¨ Mannualy Log Test Results to W&B\n",
    "    wandb.log({\n",
    "        \"test_score\": 1.4578\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End your logging session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # âœ¨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FinalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
