{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install wandb -qU\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the current working directory\n",
    "notebook_dir = notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))  \n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..')) \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from binary_classifier import (get_transforms, load_data, split_data, set_seeds, \n",
    "                 verify_splits, verify_data, plot_species_grid,\n",
    "                 verify_loader_transforms)\n",
    "from binary_classifier.data_utils import ImagesDataset\n",
    "from binary_classifier.models import build_resnet50_basic, build_efficientnet_v2_basic\n",
    "from binary_classifier.train import setup_training, evaluate, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': 'cuda', 'model': {'num_classes': 2, 'architecture': 'efficientnet', 'freeze_backbone': False, 'dropout': 0.2, 'hidden_units1': 100}, 'train': {'batch_size': 16, 'epochs': 3, 'lr': 0.001, 'momentum': 0.9, 'optimizer': 'sgd', 'criterion': 'cross_entropy'}, 'experiment': {'seed': 42, 'experiment_name': 'efficientnet_single_classifier'}, 'transforms': {'resize': [480, 480], 'horizontal_flip': 0.5, 'rotate': 15, 'jitter': {'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.1}, 'custom': {'block_timestamp': True}}, 'log': {'img_count': 50}}\n"
     ]
    }
   ],
   "source": [
    "# Locate the YAML file relative to the notebook's location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# You need to update this path to your new .yaml file\n",
    "config_path = os.path.join(notebook_dir, \"../configs/default_single_ensemble_training.yaml\")\n",
    "\n",
    "# Load the YAML file\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "False\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "device = config[\"device\"]\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antelope_duiker',\n",
       " 'bird',\n",
       " 'blank',\n",
       " 'civet_genet',\n",
       " 'hog',\n",
       " 'leopard',\n",
       " 'monkey_prosimian',\n",
       " 'rodent']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, test_features, train_labels, species_labels = load_data()\n",
    "species_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_antelope_duiker = train_labels[\"antelope_duiker\"]\n",
    "train_labels_bird = train_labels[\"bird\"]\n",
    "train_labels_blank = train_labels[\"blank\"]\n",
    "train_labels_civet_genet = train_labels[\"civet_genet\"]\n",
    "train_labels_hog = train_labels[\"hog\"]\n",
    "train_labels_leopard = train_labels[\"leopard\"]\n",
    "train_labels_monkey_prosimian = train_labels[\"monkey_prosimian\"]\n",
    "train_labels_rodent = train_labels[\"rodent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transforms, val_transforms = get_transforms(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "\n",
    "X_train_antelope_duiker, X_val_antelope_duiker, y_train_antelope_duiker, y_val_antelope_duiker = split_data(train_features, train_labels_antelope_duiker)\n",
    "\n",
    "X_train_bird, X_val_bird, y_train_bird, y_val_bird = split_data(train_features, train_labels_bird, type='sites')\n",
    "\n",
    "X_train_blank, X_val_blank, y_train_blank, y_val_blank = split_data(train_features, train_labels_blank, type='sites')\n",
    "\n",
    "X_train_civet_genet, X_val_civet_genet, y_train_civet_genet, y_val_civet_genet = split_data(train_features, train_labels_civet_genet, type='sites')\n",
    "\n",
    "X_train_hog, X_val_hog, y_train_hog, y_val_hog = split_data(train_features, train_labels_hog, type='sites')\n",
    "\n",
    "X_train_leopard, X_val_leopard, y_train_leopard, y_val_leopard = split_data(train_features, train_labels_leopard, type='sites')\n",
    "\n",
    "X_train_monkey_prosimian, X_val_monkey_prosimian, y_train_monkey_prosimian, y_val_monkey_prosimian = split_data(train_features, train_labels_monkey_prosimian, type='sites')\n",
    "\n",
    "X_train_rodent, X_val_rodent, y_train_rodent, y_val_rodent = split_data(train_features, train_labels_rodent, type='sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "\n",
    "# Create datasets antelope\n",
    "train_dataset_antelope_duiker = ImagesDataset(\n",
    "    features=X_train_antelope_duiker[0:100], \n",
    "    labels=y_train_antelope_duiker[0:100], \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "\n",
    "val_dataset_antelope_duiker = ImagesDataset(\n",
    "    features=X_val_antelope_duiker[0:100], \n",
    "    labels=y_val_antelope_duiker[0:100], \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_antelope_duiker = DataLoader(\n",
    "    train_dataset_antelope_duiker, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    #num_workers=4,\n",
    "    pin_memory=False)\n",
    "\n",
    "val_loader_antelope_duiker = DataLoader(\n",
    "    val_dataset_antelope_duiker, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    #num_workers=4,\n",
    "    pin_memory=False)\n",
    "\n",
    "model_antelope_duiker = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_antelope_duiker = model_antelope_duiker.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets bird\n",
    "train_dataset_bird = ImagesDataset(\n",
    "    features=X_train_bird,\n",
    "    labels=y_train_bird,, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_bird, = ImagesDataset(\n",
    "    features=X_val_bird, \n",
    "    labels=y_val_bird, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_bird = DataLoader(\n",
    "    train_dataset_bird, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_bird = DataLoader(\n",
    "    val_dataset_bird, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "model_bird = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_bird = model_bird.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets blank\n",
    "train_dataset_blank = ImagesDataset(\n",
    "    features=X_train_blank, \n",
    "    labels=y_train_blank, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_blank = ImagesDataset(\n",
    "    features=X_val_blank, \n",
    "    labels=y_val_blank, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_blank = DataLoader(\n",
    "    train_dataset_blank, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_blank = DataLoader(\n",
    "    val_dataset_blank, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "model_blank = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_blank = model_blank.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets civet genet\n",
    "train_dataset_civet_genet = ImagesDataset(\n",
    "    features=X_train_civet_genet, \n",
    "    labels=y_train_civet_genet, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_civet_genet = ImagesDataset(\n",
    "    features=X_val_civet_genet, \n",
    "    labels=y_val_civet_genet, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_civet_genet = DataLoader(\n",
    "    train_dataset_civet_genet, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_civet_genet = DataLoader(\n",
    "    val_dataset_civet_genet, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "model_civet_genet = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_civet_genet = model_civet_genet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets hog\n",
    "train_dataset_hog = ImagesDataset(\n",
    "    features=X_train_hog, \n",
    "    labels=y_train_hog, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_hog = ImagesDataset(\n",
    "    features=X_val_hog, \n",
    "    labels=y_val_hog, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_hog = DataLoader(\n",
    "    train_dataset_hog, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_hog = DataLoader(\n",
    "    val_dataset_hog, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "model_hog = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_hog = model_hog.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets leopard\n",
    "train_dataset_leopard = ImagesDataset(\n",
    "    features=X_train_leopard, \n",
    "    labels=y_train_leopard, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_leopard = ImagesDataset(\n",
    "    features=X_val_leopard, \n",
    "    labels=y_val_leopard, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_leopard = DataLoader(\n",
    "    train_dataset_leopard, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_leopard = DataLoader(\n",
    "    val_dataset_leopard, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "model_leopard = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_leopard = model_leopard.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets monkey prosimian\n",
    "train_dataset_monkey_prosimian = ImagesDataset(\n",
    "    features=X_train_monkey_prosimian, \n",
    "    labels=y_train_monkey_prosimian, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_monkey_prosimian = ImagesDataset(\n",
    "    features=X_val_monkey_prosimian, \n",
    "    labels=y_val_monkey_prosimian, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_monkey_prosimian = DataLoader(\n",
    "    train_dataset_monkey_prosimian, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader_monkey_prosimian = DataLoader(\n",
    "    val_dataset_monkey_prosimian, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "model_monkey_prosimian = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_monkey_prosimian = model_monkey_prosimian.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets rodent\n",
    "train_dataset_rodent = ImagesDataset(\n",
    "    features=X_train_rodent, \n",
    "    labels=y_train_rodent, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset_rodent = ImagesDataset(\n",
    "    features=X_val_rodent, \n",
    "    labels=y_val_rodent, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_rodent = DataLoader(\n",
    "    train_dataset_rodent, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_rodent, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "model_rodent = build_efficientnet_v2_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model_rodent = model_rodent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_antelope_duiker\n",
    "train_loader = train_loader_antelope_duiker\n",
    "val_loader = val_loader_antelope_duiker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkseeger7\u001b[0m (\u001b[33mkseeger7-georgia-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.require()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kseeg\\Documents\\Georgia Tech\\Deep Learning\\Final Project\\Github Code\\wildlife\\notebooks\\wandb\\run-20241204_204849-4trtjiv9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife/runs/4trtjiv9' target=\"_blank\">worldly-frog-26</a></strong> to <a href='https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife' target=\"_blank\">https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife/runs/4trtjiv9' target=\"_blank\">https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife/runs/4trtjiv9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kseeger7-georgia-institute-of-technology/wildlife/runs/4trtjiv9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x235b0913410>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"wildlife\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "criterion, optimizer = setup_training(\n",
    "        model, \n",
    "        criterion=config[\"train\"][\"criterion\"],\n",
    "        optimizer=config[\"train\"][\"optimizer\"], \n",
    "        lr=config[\"train\"][\"lr\"], \n",
    "        momentum=config[\"train\"][\"momentum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     avg_train_loss, tracking_loss \u001b[38;5;241m=\u001b[39m train(model, \n\u001b[0;32m     11\u001b[0m                                      train_loader, \n\u001b[0;32m     12\u001b[0m                                      criterion, \n\u001b[0;32m     13\u001b[0m                                      optimizer, \n\u001b[0;32m     14\u001b[0m                                      epoch, config, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     15\u001b[0m     tracking_loss_all\u001b[38;5;241m.\u001b[39mextend(tracking_loss)  \u001b[38;5;66;03m# Append to global list\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)  \u001b[38;5;66;03m# Store avg training loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\Documents\\Georgia Tech\\Deep Learning\\Final Project\\Github Code\\wildlife\\binary_classifier\\train.py:55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, epoch, config, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     56\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     57\u001b[0m tracking_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Store batch loss for tracking \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kseeg\\anaconda3\\envs\\FinalProject\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Long"
     ]
    }
   ],
   "source": [
    "log_counter = 0\n",
    "tracking_loss_all = []\n",
    "train_losses = []  # To store average training loss per epoch\n",
    "val_losses = []    # To store validation loss per epoch\n",
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    # Training step\n",
    "    avg_train_loss, tracking_loss = train(model, \n",
    "                                     train_loader, \n",
    "                                     criterion, \n",
    "                                     optimizer, \n",
    "                                     epoch, config, device=device)\n",
    "    tracking_loss_all.extend(tracking_loss)  # Append to global list\n",
    "    train_losses.append(avg_train_loss)  # Store avg training loss\n",
    "    print(f\"Epoch {epoch+1}/{config['train']['epochs']} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation step\n",
    "    eval_metrics = evaluate(model, val_loader, criterion, config, epoch= epoch+1, device=device)\n",
    "    val_losses.append(eval_metrics[\"loss\"])  # Store validation loss\n",
    "    print(f\"Epoch {epoch+1}/{config['train']['epochs']} - Eval Loss: {eval_metrics['loss']:.4f}, Eval Acc: {eval_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "wandb.log({\"duration\": duration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
