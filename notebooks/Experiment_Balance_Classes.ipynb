{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6y23yCuMBwj",
    "tags": []
   },
   "source": [
    "## Experiment_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKiaXIRuMBwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install wandb -qU\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the current working directory\n",
    "notebook_dir = notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))  \n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..')) \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from src import (get_transforms, load_data, split_data, set_seeds, \n",
    "                 verify_splits, verify_data, plot_species_grid,\n",
    "                 verify_loader_transforms)\n",
    "from src.data_utils import ImagesDataset\n",
    "from src.models import build_resnet50_basic\n",
    "from src.train import setup_training, evaluate, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your directory is set up properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree ../ -L 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree ../data/ -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set up your experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this notebook. Rename it, but keep it in `notebooks/`. To update any settings, params, and/or hyperparams make a copy of `configs/default.yaml`, rename it and call your new `.yaml` below. Be sure to keep it in `configs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': 'mps', 'model': {'num_classes': 8, 'architecture': 'resnet50', 'freeze_backbone': False, 'dropout': 0.1, 'hidden_units1': 100}, 'train': {'batch_size': 32, 'epochs': 1, 'lr': 0.001, 'momentum': 0.9, 'optimizer': 'sgd', 'criterion': 'cross_entropy'}, 'experiment': {'seed': 42, 'experiment_name': 'baseline_resnet50'}, 'transforms': {'resize': [224, 224], 'horizontal_flip': 0.5, 'rotate': 15, 'jitter': {'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.1}, 'custom': {'block_timestamp': True}}, 'log': {'img_count': 20}}\n"
     ]
    }
   ],
   "source": [
    "# Locate the YAML file relative to the notebook's location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# You need to update this path to your new .yaml file\n",
    "config_path = os.path.join(notebook_dir, \"../configs/loss_exp.yaml\")\n",
    "\n",
    "# Load the YAML file\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "device = config[\"device\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Note: your data file should be hidden in the repo (.gitignore) but make sure to set it up locally like:\n",
    "\n",
    "`wildlife/data/givens/test_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features/[images...]`\n",
    "\n",
    "`wildlife/data/givens/train_features.csv`\n",
    "\n",
    "`wildlife/data/givens/test_features.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features, test_features, train_labels, species_labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transforms, val_transforms = get_transforms(config)\n",
    "# print(train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNzMhcTGMBwp"
   },
   "source": [
    "#### Split into train and evaluation sets\n",
    "\n",
    "We need to ensure that sites are mutually exclusive between the training and validation sets, meaning no site should appear in both sets. This ensures a proper stratification based on site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "X_train, X_val, y_train, y_val = split_data(\n",
    "    train_features, train_labels, type='sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (Optional)\n",
    "# verify_splits(X_train, y_train, X_val,  y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"][\"seed\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImagesDataset(\n",
    "    features=X_train, \n",
    "    labels=y_train, \n",
    "    transform=train_transforms, \n",
    "    device=device)\n",
    "val_dataset = ImagesDataset(\n",
    "    features=X_val, \n",
    "    labels=y_val, \n",
    "    transform=val_transforms, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config[\"train\"][\"batch_size\"], \n",
    "    shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify transformations in dataloaders (Optional)\n",
    "# verify_loader_transforms(train_loader, title_type='train')\n",
    "# verify_loader_transforms(val_loader, title_type='validate')\n",
    "\n",
    "# set_seeds(config[\"experiment\"]['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes for verification (Optional)\n",
    "# print(f\"Training set: {len(train_dataset)} samples\")\n",
    "# print(f\"Validation set: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPwe5YFjMBwv",
    "tags": []
   },
   "source": [
    "### **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "Note: If you build a new model, add it to `models.py` and update the block below. And update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "model = build_resnet50_basic(\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_units1 = config[\"model\"][\"hidden_units1\"],\n",
    "    dropout = config[\"model\"][\"dropout\"] \n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your criterion and optimizer\n",
    "Note: If needed up date these in `train.py` and update your `.yaml` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yf3bCjmwMBwv",
    "outputId": "e0b47689-3576-4c08-c1f8-fb4515032200"
   },
   "outputs": [],
   "source": [
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "criterion, optimizer = setup_training(\n",
    "        model, \n",
    "        criterion=config[\"train\"][\"criterion\"],\n",
    "        optimizer=config[\"train\"][\"optimizer\"], \n",
    "        lr=config[\"train\"][\"lr\"], \n",
    "        momentum=config[\"train\"][\"momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgball30\u001b[0m (\u001b[33mgball30-georgia-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.require()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/evaball/Documents/CS_Projects/HomeWork/Gatech/wildlife/notebooks/wandb/run-20241127_212121-7zwhgdql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife/runs/7zwhgdql' target=\"_blank\">visionary-plasma-7</a></strong> to <a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife' target=\"_blank\">https://wandb.ai/gball30-georgia-institute-of-technology/wildlife</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife/runs/7zwhgdql' target=\"_blank\">https://wandb.ai/gball30-georgia-institute-of-technology/wildlife/runs/7zwhgdql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"wildlife\")\n",
    "cfg = wandb.config\n",
    "cfg.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the train / eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for epoch 1\n",
      "Epoch [1/1], Step [100/412], Loss: 1.6794\n",
      "Epoch [1/1], Step [200/412], Loss: 1.8156\n",
      "Epoch [1/1], Step [300/412], Loss: 1.4075\n",
      "Epoch [1/1], Step [400/412], Loss: 1.3835\n",
      "Epoch 1/1 - Avg Train Loss: 1.4345\n",
      "Evaluation - Loss: 1.5086, Accuracy: 40.34%, Precision: 0.42, Recall: 0.40, F1: 0.36\n",
      "Epoch 1/1 - Eval Loss: 1.5086, Eval Acc: 40.34%\n"
     ]
    }
   ],
   "source": [
    "log_counter = 0\n",
    "tracking_loss_all = []\n",
    "train_losses = []  # To store average training loss per epoch\n",
    "val_losses = []    # To store validation loss per epoch\n",
    "set_seeds(config[\"experiment\"]['seed'])\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    # Training step\n",
    "    avg_train_loss, tracking_loss = train(model, \n",
    "                                     train_loader, \n",
    "                                     criterion, \n",
    "                                     optimizer, \n",
    "                                     epoch, config, device=device)\n",
    "    tracking_loss_all.extend(tracking_loss)  # Append to global list\n",
    "    train_losses.append(avg_train_loss)  # Store avg training loss\n",
    "    print(f\"Epoch {epoch+1}/{config[\"train\"][\"epochs\"]} - Avg Train Loss: {\n",
    "        avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation step\n",
    "    eval_metrics = evaluate(model, val_loader, criterion, config, device=device)\n",
    "    val_losses.append(eval_metrics[\"loss\"])  # Store validation loss\n",
    "    print(f\"Epoch {epoch+1}/{config[\"train\"][\"epochs\"]} - Eval Loss: {\n",
    "        eval_metrics['loss']:.4f}, Eval Acc: {eval_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "wandb.log({\"duration\": duration})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done logging or you want to run the experiment again, finish with the block below. But if you think you might want to submit this run to the competition, don't finish logging until the end once you've added the competition score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_accuracy</td><td>▁</td></tr><tr><td>eval_f1</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_macro_f1</td><td>▁</td></tr><tr><td>eval_precision</td><td>▁</td></tr><tr><td>eval_recall</td><td>▁</td></tr><tr><td>f1_antelope_duiker</td><td>▁</td></tr><tr><td>f1_bird</td><td>▁</td></tr><tr><td>f1_blank</td><td>▁</td></tr><tr><td>f1_civet_genet</td><td>▁</td></tr><tr><td>f1_hog</td><td>▁</td></tr><tr><td>f1_leopard</td><td>▁</td></tr><tr><td>f1_macro avg</td><td>▁</td></tr><tr><td>f1_monkey_prosimian</td><td>▁</td></tr><tr><td>f1_rodent</td><td>▁</td></tr><tr><td>f1_weighted avg</td><td>▁</td></tr><tr><td>loss</td><td>▆▅▆▄▇▄▃▅▅▄▄▅▂▃▃▃▂▄▅▃▆█▄▃▃▃▄▄▆▃▅▄▃▃▃▂▃▁▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>319.36767</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>eval_accuracy</td><td>40.33765</td></tr><tr><td>eval_f1</td><td>0.36389</td></tr><tr><td>eval_loss</td><td>1.50865</td></tr><tr><td>eval_macro_f1</td><td>0.34414</td></tr><tr><td>eval_precision</td><td>0.41944</td></tr><tr><td>eval_recall</td><td>0.40338</td></tr><tr><td>f1_antelope_duiker</td><td>0.3125</td></tr><tr><td>f1_bird</td><td>0.08564</td></tr><tr><td>f1_blank</td><td>0.14724</td></tr><tr><td>f1_civet_genet</td><td>0.51691</td></tr><tr><td>f1_hog</td><td>0.14097</td></tr><tr><td>f1_leopard</td><td>0.69344</td></tr><tr><td>f1_macro avg</td><td>0.34414</td></tr><tr><td>f1_monkey_prosimian</td><td>0.48819</td></tr><tr><td>f1_rodent</td><td>0.36819</td></tr><tr><td>f1_weighted avg</td><td>0.36389</td></tr><tr><td>loss</td><td>1.07194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-plasma-7</strong> at: <a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife/runs/7zwhgdql' target=\"_blank\">https://wandb.ai/gball30-georgia-institute-of-technology/wildlife/runs/7zwhgdql</a><br/> View project at: <a href='https://wandb.ai/gball30-georgia-institute-of-technology/wildlife' target=\"_blank\">https://wandb.ai/gball30-georgia-institute-of-technology/wildlife</a><br/>Synced 5 W&B file(s), 0 media file(s), 214 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241127_212121-7zwhgdql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore Experiment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to explore and potentially submit your results \n",
    "explore = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(range(1, config[\"train\"][\"epochs\"]+1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Loss During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # Convert tracking_loss to a pandas Series for convenient rolling average\n",
    "    tracking_loss_series = pd.Series(tracking_loss_all)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    tracking_loss_series.plot(alpha=0.2, label=\"Batch Loss\")\n",
    "    tracking_loss_series.rolling(center=True, min_periods=1, window=10).mean().plot(\n",
    "        label=\"Loss (Moving Avg)\", linewidth=2\n",
    "    )\n",
    "    plt.xlabel(\"(Epoch, Batch)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Batch Loss During Training\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Labels from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    print(\"True labels (training):\")\n",
    "    print(y_train.idxmax(axis=1).value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True and Predicated Labels from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # Extract predictions and true labels from eval_metrics\n",
    "    all_preds = eval_metrics[\"all_preds\"]\n",
    "    all_labels = eval_metrics[\"all_labels\"]\n",
    "\n",
    "    # Convert all_preds to DataFrame and map to class names\n",
    "    preds_df = pd.DataFrame(all_preds, columns=[\"predicted_class\"])\n",
    "    preds_df[\"predicted_label\"] = preds_df[\"predicted_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Convert all_labels to DataFrame and map to class names\n",
    "    labels_df = pd.DataFrame(all_labels, columns=[\"true_class\"])\n",
    "    labels_df[\"true_label\"] = labels_df[\"true_class\"].map(\n",
    "        lambda idx: species_labels[idx]\n",
    "    )\n",
    "\n",
    "    # Combine predictions and true labels for analysis\n",
    "    results_df = pd.concat([preds_df, labels_df], axis=1)\n",
    "\n",
    "    # Display value counts for predicted and true labels\n",
    "    print(\"Predicted labels (eval):\")\n",
    "    print(results_df[\"predicted_label\"].value_counts())\n",
    "\n",
    "    print(\"\\nTrue labels (eval):\")\n",
    "    print(results_df[\"true_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:    \n",
    "    per_class_accuracy = results_df.groupby(\"true_label\").apply(\n",
    "        lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).mean(), \n",
    "    )\n",
    "    print(\"Per-Class Accuracy:\")\n",
    "    print(per_class_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "    eval_true = pd.Series(all_labels).apply(lambda x: species_labels[x])\n",
    "    eval_predictions = pd.Series(all_preds).apply(lambda x: species_labels[x])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(\n",
    "        eval_true,\n",
    "        eval_predictions,\n",
    "        ax=ax,\n",
    "        xticks_rotation=90,\n",
    "        colorbar=True,\n",
    "        normalize='true'\n",
    "    )\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Datatloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    set_seeds(config[\"experiment\"][\"seed\"])\n",
    "    test_dataset = ImagesDataset(\n",
    "        test_features, \n",
    "        transform=val_transforms, \n",
    "        device=device)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config[\"train\"][\"batch_size\"], \n",
    "        shuffle=False, pin_memory=True)\n",
    "    \n",
    "    print(f\"Test set: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    test_preds_collector = []\n",
    "\n",
    "    # put the model in eval mode so we don't update any parameters\n",
    "    model.eval()\n",
    "\n",
    "    # we aren't updating our weights so no need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_n, batch in enumerate(test_loader):\n",
    "            # run the forward step\n",
    "            images = batch[\"image\"].to(device)\n",
    "            logits = model(images)\n",
    "\n",
    "            # apply softmax so that model outputs are in range [0,1]\n",
    "            preds = F.softmax(logits, dim=1)\n",
    "\n",
    "            # store this batch's predictions in df\n",
    "            # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n",
    "            preds_df = pd.DataFrame(\n",
    "                preds.cpu().numpy(),\n",
    "                index=batch[\"image_id\"],\n",
    "                columns=species_labels,\n",
    "            )\n",
    "            test_preds_collector.append(preds_df)\n",
    "\n",
    "    submission_df = pd.concat(test_preds_collector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your submission. Update submission_number.\n",
    "\n",
    "Make sure your directory is properly set up, as both `/data` and `/results` are ignored by the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    submission_number=11\n",
    "    submission_df.index.name = 'id'\n",
    "    submission_df = submission_df.round(6)\n",
    "    submission_format_path = \"../data/givens/submission_format.csv\"\n",
    "    submission_format = pd.read_csv(submission_format_path, index_col=\"id\")\n",
    "\n",
    "\n",
    "    assert all(submission_df.index == submission_format.index)\n",
    "    assert all(submission_df.columns == submission_format.columns)\n",
    "\n",
    "    # Save submission_df for further use\n",
    "    submission_df_path = f\"../results/submissions/submission{submission_number}.csv\"\n",
    "    submission_df.to_csv(submission_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you submit update the submission score for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "     # ✨ Mannualy Log Test Results to W&B\n",
    "    wandb.log({\n",
    "        \"test_score\": 1.4578\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End your logging session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore:\n",
    "    # ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
