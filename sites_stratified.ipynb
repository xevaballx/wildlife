{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6y23yCuMBwj",
    "tags": []
   },
   "source": [
    "## Incrementally improved benchmark report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uKiaXIRuMBwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils import verify_data, plot_species_grid, verify_splits, split_data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Determine whether to use MPS (Metal Performance Shaders) or fallback to CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh this sell for each run. If not using macOS, make sure to set you seed for cuda if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x316552a30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "random.seed(seed)  # Python random seed\n",
    "np.random.seed(seed)  # NumPy random seed\n",
    "torch.manual_seed(seed)  # PyTorch CPU and CUDA seed\n",
    "\n",
    "# If using multiple threads or workers, you can optionally set:\n",
    "# torch.use_deterministic_algorithms(True)  # Enforces deterministic behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Constants\n",
    "\n",
    "This ensures consistency, so what we change in the model is logged properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 8  # Number of target classes\n",
    "\n",
    "# Constants for the model experiments and logging\n",
    "RESNET50_WEIGHTS = models.ResNet50_Weights.DEFAULT  # Pre-trained weights\n",
    "HIDDEN_UNITS1 = 100  # Number of units in the hidden fully connected layer\n",
    "\n",
    "# TRAIN_TRANSFORMS = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize to match model input\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip horizontally\n",
    "#     transforms.RandomRotation(15),  # Rotate within 15 degrees\n",
    "#     transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "#     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize using ImageNet stats\n",
    "# ])\n",
    "\n",
    "TRAIN_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# Hyperparmeters\n",
    "EPOCHS = 5\n",
    "DROPOUT_RATE = 0.1  # Dropout rate for regularization\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.09\n",
    "\n",
    "# Logging params\n",
    "NUM_BATCHES_TO_LOG = 10 # from the test data for each test step\n",
    "NUM_IMAGES_PER_BATCH = 32 # Number of images to log per test batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you add the data to the base_path on your local machine if running locally. We have a git ignore set up so the data folder synch remotely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kFk46IPKMBwo"
   },
   "outputs": [],
   "source": [
    "# Define the base path where images are stored\n",
    "base_path = \"./data/givens/\"\n",
    "train_features = pd.read_csv(f\"{base_path}train_features.csv\", index_col=\"id\")\n",
    "test_features = pd.read_csv(f\"{base_path}test_features.csv\", index_col=\"id\")\n",
    "train_labels = pd.read_csv(f\"{base_path}train_labels.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6sZMIpEMBwo"
   },
   "source": [
    "The `features` CSVs contain the image ID, filepath and site ID for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IKcKFKBhsYW",
    "outputId": "c7bdb61d-b60e-4c84-9468-26f9655d21f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_features['filepath'].dtype)\n",
    "print(test_features['filepath'].isnull().sum())\n",
    "print(train_features['filepath'].isnull().sum())  # Count any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'filepath' column with the full path to each image\n",
    "# Subdirectories for train and test images\n",
    "train_images_path = os.path.join(base_path, \"train_features\")\n",
    "test_images_path = os.path.join(base_path, \"test_features\")\n",
    "\n",
    "train_features['filepath'] = train_features.index.map(\n",
    "    lambda img_id: os.path.join(train_images_path, f\"{img_id}.jpg\"))\n",
    "\n",
    "test_features['filepath'] = test_features.index.map(\n",
    "    lambda img_id: os.path.join(test_images_path, f\"{img_id}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrHGSQGaMBwp"
   },
   "source": [
    "Store a sorted list of the labels, so that we can sort the inputs and outputs to our model in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlxx7flxMBwp",
    "outputId": "e6a1a5e9-9632-49a4-c85a-f5f4104d30df"
   },
   "outputs": [],
   "source": [
    "species_labels = sorted(train_labels.columns.unique())\n",
    "# species_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2kftnIdMBwp"
   },
   "source": [
    "### Explore the data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "fw3qfrSZMBwo",
    "outputId": "52e120a4-274f-4555-a5f7-2cb80b377b9b"
   },
   "outputs": [],
   "source": [
    "# helper function (optional)\n",
    "verify_data(train_features,test_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function (optional)\n",
    "plot_species_grid(train_features, train_labels, species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLg_-qfMBwp"
   },
   "source": [
    "There's a lot more data exploration to do. For example, you might also want to look at the distribution of image dimensions or camera trap sites. But since our primary goal here is to develop a benchmark, let's move on to the modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNzMhcTGMBwp"
   },
   "source": [
    "### Split into train and evaluation sets\n",
    "\n",
    "First, we'll need to split the images into train and eval sets. We'll put aside 20% of the data for evaluation and stratify by the target labels to ensure we have similar relative frequencies of each class in the train and eval sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge features and labels based on the index to associate labels with sites\n",
    "site_animal_distribution = train_features.merge(train_labels, left_index=True, right_index=True)\n",
    "\n",
    "site_animal_distribution = site_animal_distribution.drop(columns=[\"filepath\"])\n",
    "\n",
    "# Group by site and sum labels to calculate the distribution per site\n",
    "site_animal_distribution = site_animal_distribution.groupby('site').sum()\n",
    "\n",
    "\n",
    "# Check if site_animal_distribution has the correct structure\n",
    "print(site_animal_distribution.head())\n",
    "\n",
    "# Calculate overall label distribution (for reference, optional)\n",
    "total_distribution = train_labels.sum()\n",
    "print(total_distribution)\n",
    "\n",
    "# Sort sites by their contribution to label distribution\n",
    "site_animal_distribution['total'] = site_animal_distribution.sum(axis=1)\n",
    "sorted_sites = site_animal_distribution.sort_values(by='total', ascending=False)\n",
    "\n",
    "# Check sorted_sites for debugging\n",
    "print(sorted_sites.head())\n",
    "\n",
    "# Iteratively split sites to balance label distribution\n",
    "train_sites, val_sites = [], []\n",
    "train_distribution = pd.Series(0, index=train_labels.columns)\n",
    "val_distribution = pd.Series(0, index=train_labels.columns)\n",
    "\n",
    "for site, row in sorted_sites.iterrows():\n",
    "    if (train_distribution + row).max() <= (val_distribution + row).max():\n",
    "        train_sites.append(site)\n",
    "        train_distribution += row\n",
    "    else:\n",
    "        val_sites.append(site)\n",
    "        val_distribution += row\n",
    "\n",
    "# Check splits\n",
    "print(\"Train sites:\", train_sites)\n",
    "print(\"Validation sites:\", val_sites)\n",
    "\n",
    "# Create boolean masks for training and validation splits\n",
    "train_mask = train_features['site'].isin(train_sites)\n",
    "val_mask = train_features['site'].isin(val_sites)\n",
    "\n",
    "# Apply masks to split the features and labels\n",
    "train_features_split = train_features[train_mask]\n",
    "val_features_split = train_features[val_mask]\n",
    "train_labels_split = train_labels.loc[train_features_split.index]\n",
    "val_labels_split = train_labels.loc[val_features_split.index]\n",
    "\n",
    "# Check distributions\n",
    "print(\"Training Distribution:\\n\", train_distribution / train_distribution.sum())\n",
    "print(\"Validation Distribution:\\n\", val_distribution / val_distribution.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(\n",
    "    train_features, train_labels, type='sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "                                           filepath   site\n",
      "id                                                        \n",
      "ZJ000000  ./data/givens/train_features/ZJ000000.jpg  S0120\n",
      "ZJ000001  ./data/givens/train_features/ZJ000001.jpg  S0069\n",
      "ZJ000002  ./data/givens/train_features/ZJ000002.jpg  S0009\n",
      "ZJ000003  ./data/givens/train_features/ZJ000003.jpg  S0008\n",
      "ZJ000004  ./data/givens/train_features/ZJ000004.jpg  S0036\n",
      "\n",
      " y_train\n",
      "          antelope_duiker  bird  blank  civet_genet  hog  leopard  \\\n",
      "id                                                                  \n",
      "ZJ000000              0.0   1.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000001              0.0   0.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000002              0.0   1.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000003              0.0   0.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000004              0.0   0.0    0.0          0.0  0.0      1.0   \n",
      "\n",
      "          monkey_prosimian  rodent  \n",
      "id                                  \n",
      "ZJ000000               0.0     0.0  \n",
      "ZJ000001               1.0     0.0  \n",
      "ZJ000002               0.0     0.0  \n",
      "ZJ000003               1.0     0.0  \n",
      "ZJ000004               0.0     0.0  \n",
      "\n",
      " X_val\n",
      "                                           filepath   site\n",
      "id                                                        \n",
      "ZJ000009  ./data/givens/train_features/ZJ000009.jpg  S0059\n",
      "ZJ000011  ./data/givens/train_features/ZJ000011.jpg  S0014\n",
      "ZJ000012  ./data/givens/train_features/ZJ000012.jpg  S0080\n",
      "ZJ000017  ./data/givens/train_features/ZJ000017.jpg  S0080\n",
      "ZJ000019  ./data/givens/train_features/ZJ000019.jpg  S0097\n",
      "\n",
      " y_val\n",
      "          antelope_duiker  bird  blank  civet_genet  hog  leopard  \\\n",
      "id                                                                  \n",
      "ZJ000009              0.0   1.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000011              0.0   0.0    0.0          0.0  0.0      1.0   \n",
      "ZJ000012              0.0   0.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000017              0.0   1.0    0.0          0.0  0.0      0.0   \n",
      "ZJ000019              0.0   0.0    0.0          0.0  0.0      0.0   \n",
      "\n",
      "          monkey_prosimian  rodent  \n",
      "id                                  \n",
      "ZJ000009               0.0     0.0  \n",
      "ZJ000011               0.0     0.0  \n",
      "ZJ000012               1.0     0.0  \n",
      "ZJ000017               0.0     0.0  \n",
      "ZJ000019               1.0     0.0  \n",
      "\n",
      " Data Shape\n",
      "Train:  (13171, 2) (13171, 8) Validate:  (3317, 2) (3317, 8)\n",
      "\n",
      " Species percentages by split\n",
      "                     train      eval\n",
      "antelope_duiker   0.141751  0.182997\n",
      "bird              0.100524  0.095568\n",
      "blank             0.136664  0.124510\n",
      "civet_genet       0.156025  0.110944\n",
      "hog               0.058614  0.062104\n",
      "leopard           0.141143  0.119084\n",
      "monkey_prosimian  0.148736  0.160687\n",
      "rodent            0.116544  0.144106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper function (optional)\n",
    "verify_splits(X_train, y_train, X_val,  y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique sites\n",
    "unique_sites = train_features['site'].unique()\n",
    "\n",
    "# Split sites into two sets\n",
    "train_sites, val_sites = train_test_split(unique_sites, test_size=0.2, \n",
    "                                          random_state=42)\n",
    "\n",
    "# Create boolean masks for training and validation splits\n",
    "train_mask = train_features['site'].isin(train_sites)\n",
    "val_mask = train_features['site'].isin(val_sites)\n",
    "\n",
    "# Apply masks to split the features\n",
    "train_features_split = train_features[train_mask]\n",
    "val_features_split = train_features[val_mask]\n",
    "\n",
    "# Step 5: Use the same indices to split the labels\n",
    "train_labels_split = train_labels.loc[train_features_split.index]\n",
    "val_labels_split = train_labels.loc[val_features_split.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HANesGE0MBwq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the data (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, \n",
    "    train_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=train_labels, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that sites are mutually exclusive between the training and validation sets, meaning no site should appear in both sets. This ensures a proper stratification based on site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    \"\"\"Reads in an image, transforms pixel values, and serves\n",
    "    a dictionary containing the image id, image tensors, and label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, \n",
    "                 labels=None, \n",
    "                 transform=None, device=device):\n",
    "        self.data = features\n",
    "        self.label = labels\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_id = self.data.index[index]\n",
    "        image = Image.open(self.data.iloc[index][\"filepath\"]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {\"image_id\": image_id, \"image\": image}\n",
    "     \n",
    "        if self.label is not None:\n",
    "            label = torch.tensor(self.label.iloc[index].values, dtype=torch.float)\n",
    "            if self.device:\n",
    "                label = label.to(self.device)\n",
    "            sample[\"label\"] = label\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up transformations. Model transformations are defined at the top as constants to sync with logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize to match model input\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip horizontally\n",
    "#     transforms.RandomRotation(15),  # Rotate within 15 degrees\n",
    "#     transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "#     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize using ImageNet stats\n",
    "# ])\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#                     transforms.ToTensor(),\n",
    "#                     transforms.Normalize(\n",
    "#                         mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep validation transformation the same to reflect real world\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ensure consistent input size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize using ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImagesDataset(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    transform=TRAIN_TRANSFORMS, device=device)\n",
    "val_dataset = ImagesDataset(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    transform=val_transform, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be: \n",
    "\n",
    "Training set: 13190 samples\n",
    "\n",
    "Validation set: 3298 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes for verification\n",
    "print(f\"Training set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPwe5YFjMBwv",
    "tags": []
   },
   "source": [
    "### Training\n",
    "Current set up is pretrained ResNet50 model as our backbone. The pretrained model outputs a 2048-dimension embedding, which we will then connect to two more dense layers, with a ReLU and Dropout step in between.\n",
    "\n",
    "These final layers, defined in `model.fc`, are the new \"head\" of our model, and allow us to transform the image embeddings produced by the pretrained \"backbone\" into the 8-dimensional output required to learn the species classification task we're tackling here. Prior to redefining it below, `model.fc` would be the final, dense layer connecting the 2048-dimension embedding to a 1000-dimension output (corresponding to the 1000 ImageNet classes that the pretrained model was trained on). We will instead prepare the model for the current task by redefining `model.fc` to produce an 8-dimensional output corresponding to our 8 species classes (including blanks).\n",
    "\n",
    "We'll also add a couple more layers in between. The `ReLU` layer introduces non-linearity into the model head, in effect activating important features and suppressing noise. And the `Dropout` layer is a commonly used regularization component that randomly drops some nodes from the previous layer's outputs (10% of nodes in this case) during each training step, mitigating our risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model, loss, and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yf3bCjmwMBwv",
    "outputId": "e0b47689-3576-4c08-c1f8-fb4515032200"
   },
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=True) fixed warning\n",
    "model = models.resnet50(weights=RESNET50_WEIGHTS)\n",
    "\n",
    "# # Freeze the backbone parameters\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, HIDDEN_UNITS1),  # dense layer takes a 2048-dim input and outputs 100-dim\n",
    "    nn.ReLU(inplace=True),  # ReLU activation introduces non-linearity\n",
    "    nn.Dropout(DROPOUT_RATE),  # common technique to mitigate overfitting\n",
    "    nn.Linear(HIDDEN_UNITS1, NUM_CLASSES\n",
    "    ),  # final dense layer outputs 8-dim corresponding to our target classes\n",
    ")\n",
    "model = model.to(device)  # Move model to MPS device\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Define optimizer to only update the head's parameters\n",
    "# optimizer = optim.SGD(model.fc.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging\n",
    "\n",
    "Looking for output: True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change the model or hyperparameters for experimentation, you will need to update the constants at the top of the notebook and feed the constants into the config file below. You'll need to use constants in the model too to be sure model and logs are synced. \n",
    "\n",
    "Keep `wandb.init(project=\"wildlife\")` to log to our shared project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"wildlife\")\n",
    "\n",
    "# ✨ W&B: Log anything we want to experiment with here\n",
    "cfg = wandb.config\n",
    "cfg.update({\"epochs\" : EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "            \"model\": \"ResNet50\", \"weights\": RESNET50_WEIGHTS,\n",
    "            \"hidden_units1\" : HIDDEN_UNITS1, \"dropout\": DROPOUT_RATE,\n",
    "            \"lr\": LR, \"momentum\": MOMENTUM,\n",
    "            \"train_transforms\": TRAIN_TRANSFORMS, \n",
    "            # num images being logged\n",
    "            \"img_count\" : min(50, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_test_predictions(images, \n",
    "                         image_ids,\n",
    "                         labels, \n",
    "                         outputs, \n",
    "                         probs,\n",
    "                         predicted, \n",
    "                         test_table, \n",
    "                         counter):\n",
    "                        #  preds_collector):\n",
    "    \"\"\"\n",
    "    Log predictions, confidence scores, and true labels for a batch of test \n",
    "        images. Creating a table of results in W&B. It visualizes image \n",
    "        misclassifications.\n",
    "    Args:\n",
    "        images (torch.Tensor): Batch of test images.\n",
    "        image_ids (list): List of IDs corresponding to the test images.\n",
    "        labels (torch.Tensor): Ground-truth labels.\n",
    "        outputs (torch.Tensor): Model outputs (logits).\n",
    "        predicted (torch.Tensor): Predicted class indices.\n",
    "        test_table (wandb.Table): W&B Table to log predictions.\n",
    "        counter (int): Current count of images logged.\n",
    "    \"\"\"\n",
    "    # Obtain confidence scores for all classes\n",
    "    # scores = F.softmax(outputs.data, dim=1)\n",
    "    log_scores = probs.cpu().numpy()  # Convert to NumPy for logging\n",
    "    log_images = images.cpu().numpy()\n",
    "    log_labels = labels.argmax(axis=1).cpu().numpy()\n",
    "    log_preds = predicted.cpu().numpy()\n",
    "\n",
    "    # Add data to W&B table\n",
    "    for img, img_id, true_label, pred, conf_scores in zip(\n",
    "        log_images, image_ids, log_labels, log_preds, log_scores\n",
    "    ):\n",
    "        if counter >= cfg[\"img_count\"]:\n",
    "            break  # Stop logging once the limit is reached\n",
    "\n",
    "        # Convert image from [C, H, W] to [H, W, C]\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        # Add data to the W&B table\n",
    "        test_table.add_data(\n",
    "            img_id,              # Image_ID\n",
    "            wandb.Image(img),    # Image\n",
    "            pred,                # Predicted class\n",
    "            true_label,          # Ground-truth label\n",
    "            *conf_scores         # Confidence scores for all classes\n",
    "        )\n",
    "        counter += 1  # Increment the counter\n",
    "\n",
    "        # return preds_collector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XwuufhOMBwv"
   },
   "source": [
    "#### Set up training Loop\n",
    "We're now ready to train our model!\n",
    "\n",
    "We'll start simple and just run it for one epoch, but feel free to run it for more `num_epochs` if you've got the time. We hope to see a decreasing loss as training progresses, which will provide some evidence that the model is learning. Note that we haven't frozen any weights in the pretrained model, a choice which you may want to revisit and we discuss in a little more detail below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model being trained.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer for training.\n",
    "        epoch (int): Current epoch number.\n",
    "        cfg (dict): W&B configuration.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss for all batches in the epoch. Summarizing how \n",
    "        the model is performing during this epoch\n",
    "    \"\"\"\n",
    "    print(f\"Starting training for epoch {epoch}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # total_steps = len(train_loader) # num batches in loader\n",
    "    tracking_loss = []  # List to store loss for every batch\n",
    "\n",
    "    for batch_n, batch in enumerate(train_loader):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        tracking_loss.append(loss.item())  # Store batch loss for tracking \n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # wrt\n",
    "        optimizer.step()\n",
    "\n",
    "        # ✨ W&B: Log loss over training steps, visualized in the UI live\n",
    "        wandb.log({\"loss\" : loss, \"epoch\": epoch,})\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if (batch_n + 1 ) % 100 == 0: \n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch+1, EPOCHS, batch_n+1, len(train_loader), loss.item()))\n",
    "            \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, tracking_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, log_counter):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a test/validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        cfg (dict): W&B configuration.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics including accuracy and average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    log_counter = 0\n",
    "\n",
    "    # Collect all predictions and labels for metric calculations\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ✨ Initialize the W&B table\n",
    "    test_table = wandb.Table(columns=[\n",
    "        \"image_id\", \"image\", \"predicted\", \"true_label\",\n",
    "        \"score_antelope_duiker\", \"score_bird\", \"score_blank\",\n",
    "        \"score_civet_genet\", \"score_hog\", \"score_leopard\",\n",
    "        \"score_monkey_prosimian\", \"score_rodent\"\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_n, batch in enumerate(val_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            image_ids = batch[\"image_id\"]\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)  # For logging\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1) \n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.argmax(dim=1).cpu().numpy())  # Convert one-hot to class indices\n",
    "            \n",
    "\n",
    "            # Log predictions (only log a limited number of batches)\n",
    "            if batch_n < NUM_BATCHES_TO_LOG:\n",
    "                log_test_predictions(images, image_ids, labels, \n",
    "                                     outputs,probs, predicted, \n",
    "                                     test_table,log_counter)\n",
    "                log_counter += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = 100 * correct / total\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # ✨ Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"eval_loss\": avg_loss,\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1,\n",
    "        \"eval_macro_f1\": macro_f1,\n",
    "        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=all_labels,\n",
    "            preds=all_preds,\n",
    "            class_names=[\n",
    "                \"antelope_duiker\", \"bird\", \"blank\", \"civet_genet\", \n",
    "                \"hog\", \"leopard\", \"monkey_prosimian\", \"rodent\"\n",
    "            ]\n",
    "        )\n",
    "    })\n",
    "\n",
    "    wandb.log({\"test_predictions\": test_table})\n",
    "        \n",
    "    print(f\"Evaluation - Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Train and Eval Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_counter = 0\n",
    "tracking_loss_all = []\n",
    "train_losses = []  # To store average training loss per epoch\n",
    "val_losses = []    # To store validation loss per epoch\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #Training step\n",
    "    avg_train_loss, tracking_loss = train_one_epoch(model, \n",
    "                                     train_loader, \n",
    "                                     criterion, \n",
    "                                     optimizer, \n",
    "                                     epoch)\n",
    "    tracking_loss_all.extend(tracking_loss)  # Append to global list\n",
    "    train_losses.append(avg_train_loss)  # Store avg training loss\n",
    "    print(f\"Epoch {epoch+1}/{cfg['epochs']} - Avg Train Loss: {\n",
    "        avg_train_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish logging if you are done evaluation the model, but if you plan to run the rest of the notebook don't finish until the end so you can log everything below including the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation step\n",
    "for epoch in range(EPOCHS):\n",
    "    eval_metrics = evaluate(model, val_loader, criterion, log_counter)\n",
    "    val_losses.append(eval_metrics[\"loss\"])  # Store validation loss\n",
    "    print(f\"Epoch {epoch+1}/{cfg['epochs']} - Eval Loss: {\n",
    "        eval_metrics['loss']:.4f}, Eval Acc: {eval_metrics['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl7bVTmFMBwv"
   },
   "source": [
    "Now let's plot the loss by epoch and batch. The x-axis here is a tuple of `(epoch, batch)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tracking_loss to a pandas Series for convenient rolling average\n",
    "tracking_loss_series = pd.Series(tracking_loss_all)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "tracking_loss_series.plot(alpha=0.2, label=\"Batch Loss\")\n",
    "tracking_loss_series.rolling(center=True, min_periods=1, window=10).mean().plot(\n",
    "    label=\"Loss (Moving Avg)\", linewidth=2\n",
    ")\n",
    "plt.xlabel(\"(Epoch, Batch)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Batch Loss During Training\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp95JZH-MBwv"
   },
   "source": [
    "Good news, the loss is going down! This is an encouraging start, especially since we haven't done anything fancy yet.\n",
    "\n",
    "Note: In this version of the benchmark, the loss reaches 1.85 with the different randomness and params warning. The benchmark from the competition reaches 1.3. That is surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByNFrKQtMBwv"
   },
   "outputs": [],
   "source": [
    "# torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVxXyhglMBww",
    "tags": []
   },
   "source": [
    "### Predicted labels distribution\n",
    "First let's review the species distribution we saw in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "mroK41PCMBww",
    "outputId": "8743d2d9-e5a9-4d0f-eb8d-55b7101e6f5b"
   },
   "outputs": [],
   "source": [
    "print(\"True labels (training):\")\n",
    "y_train.idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and true labels from eval_metrics\n",
    "all_preds = eval_metrics[\"all_preds\"]\n",
    "all_labels = eval_metrics[\"all_labels\"]\n",
    "\n",
    "# Convert all_preds to DataFrame and map to class names\n",
    "preds_df = pd.DataFrame(all_preds, columns=[\"predicted_class\"])\n",
    "preds_df[\"predicted_label\"] = preds_df[\"predicted_class\"].map(\n",
    "    lambda idx: species_labels[idx]\n",
    ")\n",
    "\n",
    "# Convert all_labels to DataFrame and map to class names\n",
    "labels_df = pd.DataFrame(all_labels, columns=[\"true_class\"])\n",
    "labels_df[\"true_label\"] = labels_df[\"true_class\"].map(\n",
    "    lambda idx: species_labels[idx]\n",
    ")\n",
    "\n",
    "# Combine predictions and true labels for analysis\n",
    "results_df = pd.concat([preds_df, labels_df], axis=1)\n",
    "\n",
    "# Display value counts for predicted and true labels\n",
    "print(\"Predicted labels (eval):\")\n",
    "print(results_df[\"predicted_label\"].value_counts())\n",
    "\n",
    "print(\"\\nTrue labels (eval):\")\n",
    "print(results_df[\"true_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu2sSN7IMBww"
   },
   "source": [
    "### Accuracy\n",
    "Now let's compute how accurate our model is and compare that against some trivial baseline models. First let's get the labels with the highest score for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN_v3KrmMBww"
   },
   "source": [
    "Random guessing across 8 classes would yield an accuracy of 12.5% (1/8). But we could construct a slightly better trivial model by always guessing the most common class (\"monkey_prosimian\" images in this case).\n",
    "\n",
    "If we were to always guess that an image is `monkey_prosimian`, we could achieve accuracy of 15.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqHjEQQEMBww",
    "outputId": "24d44f60-984b-43ee-f8aa-86af6fc568c2"
   },
   "outputs": [],
   "source": [
    "# Convert all_labels (true labels) to their class names\n",
    "eval_true = pd.Series(all_labels).apply(lambda x: species_labels[x])\n",
    "\n",
    "# Simulate always guessing \"monkey_prosimian\"\n",
    "monkey_predictions = pd.Series(\"monkey_prosimian\", index=eval_true.index)\n",
    "\n",
    "# Compute the accuracy of always predicting \"monkey_prosimian\"\n",
    "benchmark_accuracy = (eval_true == monkey_predictions).sum() / len(monkey_predictions)\n",
    "\n",
    "print(f\"Benchmark Accuracy (always guessing 'monkey_prosimian'): {benchmark_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_predictions = pd.Series(all_preds).apply(lambda x: species_labels[x])\n",
    "\n",
    "# Calculate model accuracy\n",
    "model_accuracy = (eval_predictions == eval_true).sum() / len(eval_true)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Accuracy: {model_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy per class\n",
    "per_class_accuracy = results_df.groupby(\"true_label\").apply(\n",
    "    lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).mean()\n",
    ")\n",
    "print(\"Per-Class Accuracy:\")\n",
    "print(per_class_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx4Q6fugMBww"
   },
   "source": [
    "Let's look at the predictions from another angle.\n",
    "\n",
    "We can see from the confusion matrix below that our model does reasonably well on some species, but we have plenty of room for improvement on antelopes, birds, hogs and blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Convert predictions to species names\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    eval_true,\n",
    "    eval_predictions,\n",
    "    ax=ax,\n",
    "    xticks_rotation=90,\n",
    "    colorbar=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8suCHLTEMBww"
   },
   "source": [
    "That's where you come in! What can you do to improve on this benchmark?\n",
    "\n",
    "Here are some ideas you might want to try:\n",
    "* Train for more epochs. We've only done 1 so far.\n",
    "* Try another pretrained model. For example, you may have more success with EfficientNet, or another ResNet model with more layers like ResNet152. See what's available from pytorch [here](https://pytorch.org/vision/stable/models.html). You may also want to review which models are or have been state of the art for image classification tasks, for example on [paperswithcode.com](https://paperswithcode.com/task/image-classification). Keep in mind that different models will require different input and output dimensions, so you'll need to update how you construct `model` above.\n",
    "* Experiment with different loss functions.\n",
    "* Experiment with different learning rates or learning rate schedulers.\n",
    "* Add more layers to the model head (`model.fc`).\n",
    "* You also may want to consider freezing the weights in the backbone model and only training the head (`model.fc`). If this results in higher accuracy, that suggests the current approach may be overwriting the backbone weights in a problematic way. One approach here would be to train just the model head, and then unfreeze the backbone but train at a lower learning rate.\n",
    "* As you become more comfortable iterating through different versions of the model, you may want to try out [PyTorch Lightning](https://www.pytorchlightning.ai/) or [Lightning Flash](https://lightning-flash.readthedocs.io/en/latest/quickstart.html), which build upon PyTorch and eliminate a lot of boilerplate code, in addition to providing a more complete research framework for deep learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg9QTTxKMBwx"
   },
   "source": [
    "## 8. Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76a6DRx_MBwx"
   },
   "source": [
    "Last but not least, we'll want to participate in the competition and see where we stand on the leaderboard.\n",
    "\n",
    "To do this we need to create predictions for the **competition test set** (not the eval set we used above). You don't have labels for these.\n",
    "\n",
    "We'll create predictions in the same way we did for the `eval` set, but this time using the `test_features` we downloaded from the [competition website](https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X83p0e6kMBwx"
   },
   "outputs": [],
   "source": [
    "# test_dataset = ImagesDataset(test_features.filepath.to_frame())\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImagesDataset(\n",
    "    test_features, \n",
    "    transform=val_transform, \n",
    "    device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "yz08YKCrMBwx",
    "outputId": "6a79bdbb-ebff-4dd5-c21b-d7710c68ceb4"
   },
   "outputs": [],
   "source": [
    "test_preds_collector = []\n",
    "\n",
    "# put the model in eval mode so we don't update any parameters\n",
    "model.eval()\n",
    "\n",
    "# we aren't updating our weights so no need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    for batch_n, batch in enumerate(test_loader):\n",
    "        # run the forward step\n",
    "        images = batch[\"image\"].to(device)\n",
    "        logits = model(images)\n",
    "\n",
    "        # apply softmax so that model outputs are in range [0,1]\n",
    "        preds = F.softmax(logits, dim=1)\n",
    "\n",
    "        # store this batch's predictions in df\n",
    "        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n",
    "        preds_df = pd.DataFrame(\n",
    "            preds.cpu().numpy(),\n",
    "            index=batch[\"image_id\"],\n",
    "            columns=species_labels,\n",
    "        )\n",
    "        test_preds_collector.append(preds_df)\n",
    "\n",
    "submission_df = pd.concat(test_preds_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'id' as the index name\n",
    "submission_df.index.name = 'id'\n",
    "submission_df = submission_df.round(6)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf7PXcg8MBwx"
   },
   "source": [
    "Let's check a couple things on `submission_df` before submitting to the platform. We'll want to make sure our submission's index and column labels match the submission format. (The DrivenData platform will do these data integrity checks as well, but it will be quicker to detect problems this way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cRBuX8fMBwx"
   },
   "outputs": [],
   "source": [
    "submission_format_path = \"./data/givens/submission_format.csv\"\n",
    "submission_format = pd.read_csv(submission_format_path, index_col=\"id\")\n",
    "\n",
    "\n",
    "assert all(submission_df.index == submission_format.index)\n",
    "assert all(submission_df.columns == submission_format.columns)\n",
    "\n",
    "# Save submission_df for further use\n",
    "submission_df_path = \"./data/submissions/submission6.csv\"\n",
    "submission_df.to_csv(submission_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdAFuw5-MBwx"
   },
   "source": [
    "Looks like we're ready to submit! Save the dataframe out to a CSV file and then upload it via the [Submissions page](https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/submissions/) on the competition website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ W&B: Mark the run as complete (Or wait until the end of notebook)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
