{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to implement u-net background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision import utils as tvu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device to run on\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "if(torch.backends.mps.is_available()):\n",
    "    device = \"mps\"\n",
    "elif(torch.cuda.is_available()):\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Running on {device}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear out gpu memory:\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed value for repeatability\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)  # Python's random module\n",
    "torch.manual_seed(seed)  # PyTorch CPU/GPU seed\n",
    "np.random.seed(seed)  # NumPy random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loading up the data\n",
    "\n",
    "# TODO: update base_path to point to wherever your data is stored\n",
    "base_path = \"../../Data/data/\"\n",
    "\n",
    "train_features = pd.read_csv(f\"{base_path}train_features.csv\", index_col=\"id\")\n",
    "test_features = pd.read_csv(f\"{base_path}test_features.csv\", index_col=\"id\")\n",
    "train_labels = pd.read_csv(f\"{base_path}train_labels.csv\", index_col=\"id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'filepath' column with the full path to each image\n",
    "# Subdirectories for train and test images\n",
    "train_images_path = os.path.join(base_path, \"train_features\")\n",
    "test_images_path = os.path.join(base_path, \"test_features\")\n",
    "\n",
    "train_features['filepath'] = train_features.index.map(\n",
    "    lambda img_id: os.path.join(train_images_path, f\"{img_id}.jpg\"))\n",
    "\n",
    "test_features['filepath'] = test_features.index.map(\n",
    "    lambda img_id: os.path.join(test_images_path, f\"{img_id}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_labels = sorted(train_labels.columns.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start diverging from the \"standard\" example provided by the competition here. Let's pull out all the blank images from the train dataset. Then we'll test/train split on that for training the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_training_labels = train_labels[train_labels[\"blank\"]==1]\n",
    "\n",
    "blank_training_images = train_features.loc[blank_training_labels.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only using one class here, this is more of an unsupervised activity than a supervised one. As such, we only have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(blank_training_images, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.loc['ZJ006397', \"filepath\"])\n",
    "\n",
    "print(X_train.index[874])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose batch size here:\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class BackgroundDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_features, transform = None, device=device):\n",
    "        self.data = train_features\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.data.index[index]\n",
    "        image = Image.open(self.data.loc[image_id,\"filepath\"]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {\"image_id\": image_id, \"image\": image}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "background_transform_input = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "background_transform_target = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "input_dataset = BackgroundDataset(X_train, transform=background_transform_input, device=device)\n",
    "target_dataset =BackgroundDataset(X_train, transform=background_transform_target, device=device)\n",
    "test_dataset =  BackgroundDataset(X_test, transform= background_transform_target, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pin = False if device==\"cuda\" else True\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    input_dataset,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    shuffle = False,\n",
    "    pin_memory = pin\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    pin_memory = pin\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets start working on building the U-Net itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guided by: https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
    "\n",
    "class DownConvolve(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1), #need to add padding so our input doesn't become too small through encoder\n",
    "            nn.BatchNorm2d(out_channels), # batch norms not in original paper, but adding for grad flow\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1), #need to add padding so our input doesn't become too small through encoder, actually, mayeb we just need to remove a layer\n",
    "            nn.BatchNorm2d(out_channels), # batch norms not in original paper, but adding for grad flow\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            return self.down(x)\n",
    "\n",
    "class UpConvolve(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.in_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(in_channels), # batch norms not in original paper, but adding for grad flow\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(self.in_channels, self.in_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(in_channels), # batch norms not in original paper, but adding for grad flow\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "\n",
    "        x = self.up(x)\n",
    "\n",
    "        # need to adjust x so that it \"fits\" with the tensor from skip connection\n",
    "        height_x, width_x = x.shape[2], x.shape[3]\n",
    "        height_skip_x, width_skip_x = skip_x.shape[2], skip_x.shape[3]\n",
    "        height_diff = height_skip_x - height_x\n",
    "        width_diff = width_skip_x - width_x\n",
    "\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        #print(f\"Residual shape: {skip_x.shape}\")\n",
    "        #print(f\"height difference: {height_diff}, padding by: {height_diff//2}\")\n",
    "        #print(f\"weidth difference: {width_diff}, padding by: {width_diff//2}\")\n",
    "\n",
    "        padded_x = F.pad(x, (width_diff//2, width_diff//2, height_diff//2, height_diff//2))\n",
    "        \n",
    "        #print(f\"Skip X has size: {skip_x.shape}, Padded_x has shape: {padded_x.shape}\")\n",
    "\n",
    "        out = torch.cat((skip_x, padded_x), dim=1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel_count, output_class_count):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_channel_count = input_channel_count\n",
    "        self.output_channel_count = output_class_count\n",
    "\n",
    "        self.final_up_conv = nn.ConvTranspose2d(128, self.output_channel_count, kernel_size=2, stride=2)\n",
    "\n",
    "        self.down_layer_1 = DownConvolve(self.input_channel_count, 64)\n",
    "        self.down_layer_2 = DownConvolve(64, 128)\n",
    "        self.down_layer_3 = DownConvolve(128, 256)\n",
    "        self.down_layer_4 = DownConvolve(256, 512)\n",
    "        self.down_layer_5 = DownConvolve(512, 1024)\n",
    "\n",
    "        self.up_layer_1 = UpConvolve(1024, 512)\n",
    "        self.up_layer_2 = UpConvolve(512*2, 256)\n",
    "        self.up_layer_3 = UpConvolve(256*2, 128)\n",
    "        self.up_layer_4 = UpConvolve(128*2, 64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(f\"Shape of x: {x.shape}\")\n",
    "        d1 = self.down_layer_1(x)\n",
    "        #print(f\"Shape of d1: {d1.shape}\")\n",
    "        d2 = self.down_layer_2(d1)\n",
    "        #print(f\"Shape of d2: {d2.shape}\")\n",
    "        d3 = self.down_layer_3(d2)\n",
    "        #print(f\"Shape of d3 is: {d3.shape}\")\n",
    "        d4 = self.down_layer_4(d3)\n",
    "        #print(f\"Shape of d4 is: {d4.shape}\")\n",
    "        d5 = self.down_layer_5(d4)\n",
    "        #print(f\"Shape of d5 is: {d5.shape}\")\n",
    "\n",
    "        u1 = self.up_layer_1(d5, d4)\n",
    "        #print(f\"Shape of u1: {u1.shape}\")\n",
    "        u2 = self.up_layer_2(u1, d3) \n",
    "        #print(f\"Shape of u2: {u2.shape}\")\n",
    "        u3 = self.up_layer_3(u2, d2)\n",
    "        #print(f\"Shape of u3: {u3.shape}\")\n",
    "        u4 = self.up_layer_4(u3, d1)\n",
    "        #print(f\"Shape of u4: {u4.shape}\")\n",
    "\n",
    "        out = self.final_up_conv(u4)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BETA_VALUES = (0.9, 0.99)\n",
    "EPOCHS=3\n",
    "\n",
    "\n",
    "\n",
    "unet = UnetModel(3,3).to(device)\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = optim.Adam(unet.parameters(), lr=LEARNING_RATE, betas=BETA_VALUES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, input_transform, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model being trained.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer for training.\n",
    "        epoch (int): Current epoch number.\n",
    "        cfg (dict): W&B configuration.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss for all batches in the epoch. Summarizing how \n",
    "        the model is performing during this epoch\n",
    "    \"\"\"\n",
    "    print(f\"Starting training for epoch {epoch}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # total_steps = len(train_loader) # num batches in loader\n",
    "    tracking_loss = []  # List to store loss for every batch\n",
    "\n",
    "    for batch_n, batch in enumerate(train_loader):\n",
    "        images = input_transform(batch[\"image\"]).to(device)\n",
    "        labels = batch[\"image\"].to(device) # to train this, we use the empty frames as both inputs and ground truth labels\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        tracking_loss.append(loss.item())  # Store batch loss for tracking \n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # wrt\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (batch_n + 1 ) % 10 == 0: \n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch+1, EPOCHS, batch_n+1, len(train_loader), loss.item()))\n",
    "            \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    return avg_loss, tracking_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, log_counter):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a test/validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        cfg (dict): W&B configuration.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics including accuracy and average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    log_counter = 0\n",
    "\n",
    "    # Collect all predictions and labels for metric calculations\n",
    "    all_outputs = []\n",
    "    all_ids = []\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_n, batch in enumerate(val_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"image\"].to(device)\n",
    "            image_ids = batch[\"image_id\"]\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_ids.append(all_ids)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f\"Evaluation - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return all_outputs[:-5], all_ids[:-5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_counter = 0\n",
    "tracking_loss_all = []\n",
    "#t = transforms.RandomErasing(p=1, scale=(0.02, 0.9), ratio=(0.03, 3.3), value=(np.random.randint(0,255),np.random.randint(0,255),np.random.randint(0,255)))\n",
    "t = transforms.RandomErasing(p=1, scale=(0.02, 0.9), ratio=(0.03, 3.3), value=(0))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #Training step\n",
    "    avg_train_loss, tracking_loss = train_one_epoch(unet, \n",
    "                                     train_loader,\n",
    "                                     t,\n",
    "                                     criteria, \n",
    "                                     optimizer, \n",
    "                                     epoch)\n",
    "    tracking_loss_all.extend(tracking_loss)  # Append to global list\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), \"trained_unet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation step\n",
    "backgrounds, ids = evaluate(unet, test_loader, criteria, log_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = backgrounds[0]\n",
    "print(f\"Shape of backgrounds is: {len(backgrounds)}\")\n",
    "\n",
    "def prep_picture(image_tensor, index):\n",
    "    im = image_tensor[index,:,:,:]\n",
    "    image = np.transpose(im.cpu().numpy(), (1,2,0))\n",
    "    min_val = image.min()\n",
    "    max_val = image.max()\n",
    "    normalized_image = (image-min_val)/(max_val-min_val)\n",
    "    return normalized_image\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,20))\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    print(idx)\n",
    "    image = prep_picture(im, idx)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"Generated Background {idx}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this if loading pretrained model:\n",
    "\n",
    "unet = UnetModel(3,3)\n",
    "unet.load_state_dict(torch.load(\"trained_unet.pth\"))\n",
    "unet.eval()\n",
    "unet.to(device)\n",
    "\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingData(Dataset):\n",
    "\n",
    "    def __init__(self, train_features, transform = None, device=device):\n",
    "        self.data = train_features\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.data.index[index]\n",
    "        image = Image.open(self.data.loc[image_id,\"filepath\"]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {\"image_id\": image_id, \"image\": image}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "preprocessing_transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PreprocessingData(train_features, transform=preprocessing_transform, device=device)\n",
    "test_dataset =  PreprocessingData(test_features,  transform=preprocessing_transform, device=device)\n",
    "\n",
    "\n",
    "pin = False if device==\"cuda\" else True\n",
    "\n",
    "batch_size_train = len(train_dataset)\n",
    "batch_size_test =  len(test_dataset)\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False,\n",
    "    pin_memory = pin\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    pin_memory = pin\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_unet(train_loader, pre_trained_model):\n",
    "    \n",
    "    preprocessed_train_data = []\n",
    "    preprocessed_train_labels = []\n",
    "    backgrounds = []\n",
    "\n",
    "    trans = transforms.Resize((224, 224))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, datapoint in enumerate(train_loader):\n",
    "            data = datapoint[\"image\"].to(device)\n",
    "            id = datapoint[\"image_id\"]\n",
    "            out = pre_trained_model(data)\n",
    "            resized_out = trans(out)\n",
    "            background_subtract = torch.subtract(trans(data), resized_out)\n",
    "            preprocessed_train_data.append(background_subtract)\n",
    "            preprocessed_train_labels.append(id)\n",
    "            backgrounds.append(resized_out)\n",
    "            torch.cuda.empty_cache()\n",
    "            path = base_path+\"/preprocessed_training_images/{}\".format(preprocessed_train_labels[idx][0])+\".jpg\"\n",
    "            tvu.save_image(background_subtract, path)\n",
    "            path = base_path+\"/preprocessed_training_backgrounds/{}\".format(preprocessed_train_labels[idx][0])+\".jpg\"\n",
    "            tvu.save_image(backgrounds[idx], path)\n",
    "\n",
    "            if (idx + 1 ) % 100 == 0:\n",
    "                print(f\"Processed {idx} images...\")\n",
    "\n",
    "\n",
    "        return preprocessed_train_data, preprocessed_train_labels, backgrounds\n",
    "\n",
    "\n",
    "preprocessed_training_data, preprocessed_train_labels, backgrounds = preprocess_with_unet(test_loader, unet)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for idx, preprocessed_image in enumerate(preprocessed_training_data):\n",
    "    path = base_path+\"/preprocessed_testing_images/{}\".format(preprocessed_train_labels[idx][0])+\".jpg\"\n",
    "    tvu.save_image(preprocessed_image, path)\n",
    "    path = base_path+\"/preprocessed_testing_backgrounds/{}\".format(preprocessed_train_labels[idx][0])+\".jpg\"\n",
    "    tvu.save_image(backgrounds[idx], path)\n",
    "\n",
    "\n",
    "\n",
    "# preprocessed_test_data, preprocessed_test_labels, backgrounds = preprocess_with_unet(test_loader, unet)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# for idx, preprocessed_image in enumerate(preprocessed_test_data):\n",
    "#     path = base_path+\"/preprocessed_testing_images/{}\".format(preprocessed_test_labels[idx][0])+\".jpg\"\n",
    "#     tvu.save_image(preprocessed_image, path)\n",
    "#     path = base_path+\"/preprocessed_testing_backgrounds/{}\".format(preprocessed_test_labels[idx][0])+\".jpg\"\n",
    "#     tvu.save_image(backgrounds[idx], path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'filepath' column with the full path to each image\n",
    "# Subdirectories for train and test images\n",
    "train_images_path = os.path.join(base_path, \"preprocessed_training_images\")\n",
    "test_images_path = os.path.join(base_path, \"preprocessed_testing_images\")\n",
    "\n",
    "train_features['filepath'] = train_features.index.map(\n",
    "    lambda img_id: os.path.join(train_images_path, f\"{img_id}.jpg\"))\n",
    "\n",
    "test_features['filepath'] = test_features.index.map(\n",
    "    lambda img_id: os.path.join(test_images_path, f\"{img_id}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, lets just plug in the benchmark code to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "frac = 0.5\n",
    "\n",
    "y = train_labels.sample(frac=frac, random_state=1)\n",
    "x = train_features.loc[y.index].filepath.to_frame()\n",
    "\n",
    "# note that we are casting the species labels to an indicator/dummy matrix\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(\n",
    "    x, y, stratify=y, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    \"\"\"Reads in an image, transforms pixel values, and serves\n",
    "    a dictionary containing the image id, image tensors, and label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_df, y_df=None):\n",
    "        self.data = x_df\n",
    "        self.label = y_df\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.data.iloc[index][\"filepath\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image_id = self.data.index[index]\n",
    "        # if we don't have labels (e.g. for test set) just return the image and image id\n",
    "        if self.label is None:\n",
    "            sample = {\"image_id\": image_id, \"image\": image}\n",
    "        else:\n",
    "            label = torch.tensor(self.label.iloc[index].values,\n",
    "                                 dtype=torch.float)\n",
    "            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ImagesDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# model = models.resnet50(pretrained=True) fixed warning\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 100),  # dense layer takes a 2048-dim input and outputs 100-dim\n",
    "    nn.ReLU(inplace=True),  # ReLU activation introduces non-linearity\n",
    "    nn.Dropout(0.1),  # common technique to mitigate overfitting\n",
    "    nn.Linear(\n",
    "        100, 8\n",
    "    ),  # final dense layer outputs 8-dim corresponding to our target classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "tracking_loss = {}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    # iterate through the dataloader batches. tqdm keeps track of progress.\n",
    "    for batch_n, batch in tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader)\n",
    "    ):\n",
    "\n",
    "        # 1) zero out the parameter gradients so that gradients from previous batches are not used in this step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) run the forward step on this batch of images\n",
    "        outputs = model(batch[\"image\"])\n",
    "\n",
    "        # 3) compute the loss\n",
    "        loss = criterion(outputs, batch[\"label\"])\n",
    "        # let's keep track of the loss by epoch and batch\n",
    "        tracking_loss[(epoch, batch_n)] = float(loss)\n",
    "\n",
    "        # 4) compute our gradients\n",
    "        loss.backward()\n",
    "        # update our weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_loss = pd.Series(tracking_loss)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "tracking_loss.plot(alpha=0.2, label=\"loss\")\n",
    "tracking_loss.rolling(center=True, min_periods=1, window=10).mean().plot(\n",
    "    label=\"loss (moving avg)\"\n",
    ")\n",
    "plt.xlabel(\"(Epoch, Batch)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = ImagesDataset(x_eval, y_eval)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:06<00:00,  1.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antelope_duiker</th>\n",
       "      <th>bird</th>\n",
       "      <th>blank</th>\n",
       "      <th>civet_genet</th>\n",
       "      <th>hog</th>\n",
       "      <th>leopard</th>\n",
       "      <th>monkey_prosimian</th>\n",
       "      <th>rodent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZJ010593</th>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.079741</td>\n",
       "      <td>0.125257</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>0.069645</td>\n",
       "      <td>0.107036</td>\n",
       "      <td>0.133416</td>\n",
       "      <td>0.138623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ008057</th>\n",
       "      <td>0.135797</td>\n",
       "      <td>0.092568</td>\n",
       "      <td>0.137793</td>\n",
       "      <td>0.178094</td>\n",
       "      <td>0.083618</td>\n",
       "      <td>0.129841</td>\n",
       "      <td>0.125118</td>\n",
       "      <td>0.117170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ005326</th>\n",
       "      <td>0.151114</td>\n",
       "      <td>0.110766</td>\n",
       "      <td>0.127076</td>\n",
       "      <td>0.132084</td>\n",
       "      <td>0.088428</td>\n",
       "      <td>0.126328</td>\n",
       "      <td>0.151652</td>\n",
       "      <td>0.112552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ013073</th>\n",
       "      <td>0.082692</td>\n",
       "      <td>0.038828</td>\n",
       "      <td>0.091775</td>\n",
       "      <td>0.453112</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>0.059297</td>\n",
       "      <td>0.083809</td>\n",
       "      <td>0.160476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ008733</th>\n",
       "      <td>0.151613</td>\n",
       "      <td>0.091706</td>\n",
       "      <td>0.118579</td>\n",
       "      <td>0.162191</td>\n",
       "      <td>0.070735</td>\n",
       "      <td>0.128897</td>\n",
       "      <td>0.166322</td>\n",
       "      <td>0.109957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ013009</th>\n",
       "      <td>0.143112</td>\n",
       "      <td>0.105311</td>\n",
       "      <td>0.124249</td>\n",
       "      <td>0.143895</td>\n",
       "      <td>0.084904</td>\n",
       "      <td>0.123357</td>\n",
       "      <td>0.152951</td>\n",
       "      <td>0.122220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ008801</th>\n",
       "      <td>0.107594</td>\n",
       "      <td>0.056501</td>\n",
       "      <td>0.116386</td>\n",
       "      <td>0.316403</td>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.078193</td>\n",
       "      <td>0.111116</td>\n",
       "      <td>0.164039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ014619</th>\n",
       "      <td>0.149384</td>\n",
       "      <td>0.112647</td>\n",
       "      <td>0.130389</td>\n",
       "      <td>0.128638</td>\n",
       "      <td>0.083462</td>\n",
       "      <td>0.120323</td>\n",
       "      <td>0.159389</td>\n",
       "      <td>0.115767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ003923</th>\n",
       "      <td>0.071433</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.077357</td>\n",
       "      <td>0.539494</td>\n",
       "      <td>0.024054</td>\n",
       "      <td>0.049421</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.136802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ015012</th>\n",
       "      <td>0.153316</td>\n",
       "      <td>0.112212</td>\n",
       "      <td>0.132053</td>\n",
       "      <td>0.115774</td>\n",
       "      <td>0.080466</td>\n",
       "      <td>0.132726</td>\n",
       "      <td>0.175581</td>\n",
       "      <td>0.097871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2061 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          antelope_duiker      bird     blank  civet_genet       hog  \\\n",
       "ZJ010593         0.129832  0.079741  0.125257     0.216450  0.069645   \n",
       "ZJ008057         0.135797  0.092568  0.137793     0.178094  0.083618   \n",
       "ZJ005326         0.151114  0.110766  0.127076     0.132084  0.088428   \n",
       "ZJ013073         0.082692  0.038828  0.091775     0.453112  0.030011   \n",
       "ZJ008733         0.151613  0.091706  0.118579     0.162191  0.070735   \n",
       "...                   ...       ...       ...          ...       ...   \n",
       "ZJ013009         0.143112  0.105311  0.124249     0.143895  0.084904   \n",
       "ZJ008801         0.107594  0.056501  0.116386     0.316403  0.049768   \n",
       "ZJ014619         0.149384  0.112647  0.130389     0.128638  0.083462   \n",
       "ZJ003923         0.071433  0.030696  0.077357     0.539494  0.024054   \n",
       "ZJ015012         0.153316  0.112212  0.132053     0.115774  0.080466   \n",
       "\n",
       "           leopard  monkey_prosimian    rodent  \n",
       "ZJ010593  0.107036          0.133416  0.138623  \n",
       "ZJ008057  0.129841          0.125118  0.117170  \n",
       "ZJ005326  0.126328          0.151652  0.112552  \n",
       "ZJ013073  0.059297          0.083809  0.160476  \n",
       "ZJ008733  0.128897          0.166322  0.109957  \n",
       "...            ...               ...       ...  \n",
       "ZJ013009  0.123357          0.152951  0.122220  \n",
       "ZJ008801  0.078193          0.111116  0.164039  \n",
       "ZJ014619  0.120323          0.159389  0.115767  \n",
       "ZJ003923  0.049421          0.070742  0.136802  \n",
       "ZJ015012  0.132726          0.175581  0.097871  \n",
       "\n",
       "[2061 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "preds_collector = []\n",
    "\n",
    "# put the model in eval mode so we don't update any parameters\n",
    "model.eval()\n",
    "\n",
    "# we aren't updating our weights so no need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader)):\n",
    "        # 1) run the forward step\n",
    "        logits = model.forward(batch[\"image\"])\n",
    "        # 2) apply softmax so that model outputs are in range [0,1]\n",
    "        preds = nn.functional.softmax(logits, dim=1)\n",
    "        # 3) store this batch's predictions in df\n",
    "        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n",
    "        preds_df = pd.DataFrame(\n",
    "            preds.detach().numpy(),\n",
    "            index=batch[\"image_id\"],\n",
    "            columns=species_labels,\n",
    "        )\n",
    "        preds_collector.append(preds_df)\n",
    "\n",
    "eval_preds_df = pd.concat(preds_collector)\n",
    "eval_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels (training):\n",
      "Predicted labels (eval):\n",
      "True labels (eval):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2954876273653566"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"True labels (training):\")\n",
    "y_train.idxmax(axis=1).value_counts()\n",
    "print(\"Predicted labels (eval):\")\n",
    "eval_preds_df.idxmax(axis=1).value_counts()\n",
    "print(\"True labels (eval):\")\n",
    "y_eval.idxmax(axis=1).value_counts()\n",
    "eval_predictions = eval_preds_df.idxmax(axis=1)\n",
    "eval_predictions.head()\n",
    "eval_true = y_eval.idxmax(axis=1)\n",
    "\n",
    "(eval_true == \"monkey_prosimian\").sum() / len(eval_predictions)\n",
    "\n",
    "correct = (eval_predictions == eval_true).sum()\n",
    "accuracy = correct / len(eval_predictions)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
